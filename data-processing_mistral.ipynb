{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffce314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import dateparser\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "from mistralai import Mistral\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, get_origin, get_args, Union\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, FloatType, BooleanType, TimestampType, StructField, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bddc93",
   "metadata": {},
   "source": [
    "# SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15322ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 53.4M/53.4M [00:08<00:00, 6.01MB/s]\n",
      "Downloading data: 100%|██████████| 15.2M/15.2M [00:02<00:00, 5.17MB/s]\n",
      "Generating train split: 100%|██████████| 6241/6241 [00:00<00:00, 18368.45 examples/s]\n",
      "Generating test split: 100%|██████████| 1759/1759 [00:00<00:00, 19980.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnamuangtoun/resume-job-description-fit\")\n",
    "df = dataset[\"train\"].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf38c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91b2920-fae1-41e2-a379-688a6a23bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09600e-cc2b-49e4-bdc1-85d9111b8f99",
   "metadata": {},
   "source": [
    "Generate random snapshot dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2b7f880-f9dc-48c3-8ff6-b9e6020e35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seeded Generator\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# Define start and end date\n",
    "start_date = pd.to_datetime('2024-01-01')\n",
    "end_date = pd.to_datetime('2025-01-01')\n",
    "\n",
    "# Generate random timestamps between start_date and end_date\n",
    "random_dates = pd.to_datetime(\n",
    "    rng.uniform(start_date.value, end_date.value, size=len(df))\n",
    ")\n",
    "\n",
    "# Ensure it's treated as a pandas Series and convert to date\n",
    "df['snapshot_date'] = pd.Series(random_dates).dt.date  # This will convert to date format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66030faf-5e3d-41f8-b884-6c6897952e6d",
   "metadata": {},
   "source": [
    "Generate random IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ff68f2a-f7a7-4d7c-86e5-83c0c515bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_id(prefix: str, length=8, use_digits=True, use_letters=True, seed=42):\n",
    "    rng = np.random.default_rng(seed=seed) \n",
    "\n",
    "    characters = ''\n",
    "    \n",
    "    if use_digits:\n",
    "        characters += string.digits\n",
    "    if use_letters:\n",
    "        characters += string.ascii_letters\n",
    "\n",
    "    # Ensure we have characters to choose from\n",
    "    if not characters:\n",
    "        raise ValueError(\"At least one of 'use_digits' or 'use_letters' must be True.\")\n",
    "    \n",
    "    # Use np.random.choice to randomly select characters\n",
    "    random_id = ''.join(rng.choice(list(characters), size=length))\n",
    "    return prefix + random_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c65ff97-0bb5-4151-8a0d-2ddac097bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['resume_id'] = df.apply(lambda row: generate_random_id('RES_', seed=row.name), axis=1)\n",
    "df['job_id'] = df.apply(lambda row: generate_random_id('JD_', seed=row.name), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc7cc9",
   "metadata": {},
   "source": [
    "# BRONZE TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13edbf69",
   "metadata": {},
   "source": [
    "## Resume Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4dca9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "    \n",
    "class Experience(BaseModel):\n",
    "    role: Optional[str] = Field(None, description=\"The job title or position held\")\n",
    "    company: Optional[str] = Field(None, description=\"The name of the company. Exclude other description or location\")\n",
    "    date_start: Optional[str] = Field(None, description=\"The start date of the job. Dates must be in ISO 8601 format (YYYY-MM-DDTHH:MM:SS) or use the keywords 'present', 'current', or 'ongoing'\")\n",
    "    date_end: Optional[str] = Field(None, description=\"The end date of the job. Dates must be in ISO 8601 format (YYYY-MM-DDTHH:MM:SS) or use the keywords 'present', 'current', or 'ongoing'\")\n",
    "    role_description: Optional[str] = Field(None, description=\"A description of the responsibilities and achievements in the role\")\n",
    "\n",
    "class Education(BaseModel):\n",
    "    degree: Optional[str] = Field(None, description=\"The academic degree obtained\")\n",
    "    institution: Optional[str] = Field(None, description=\"The name of the educational institution\")\n",
    "    date_start: Optional[str] = Field(None, description=\"The start date of the education program. Dates must be in ISO 8601 format (YYYY-MM-DDTHH:MM:SS) or use the keywords 'present', 'current', or 'ongoing'\")\n",
    "    date_end: Optional[str] = Field(None, description=\"The end date of the education program. Dates must be in ISO 8601 format (YYYY-MM-DDTHH:MM:SS) or use the keywords 'present', 'current', or 'ongoing'\")\n",
    "    grade: Optional[float] = Field(None, description=\"The GPA or final grade, if available\")\n",
    "    description: Optional[str] = Field(None, description=\"Additional details about the education\")\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    name: Optional[str] = Field(None, description=\"Full name of the person\")\n",
    "    location_preference: Optional[str] = Field(None, description=\"Preference for their work location / remote, if stated\")\n",
    "    work_authorizaton: Optional[str] = Field(None, description=\"Work authorization that the person holds, such as citizenship, if stated\")\n",
    "    employment_type_preference: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Type of employment the resume is looking for such as Full-time, Part-time, Contract, Freelance, or Internship, if stated. It can also be a preference for remote work or on-site work\"\n",
    "    )\n",
    "    hard_skills: List[str] = Field(default_factory=list, description=\"A list of hard or technical skills mentioned in the resume. All hard skills are tools, frameworks, or programming languages (e.g., Python, TensorFlow, Docker). Keep it as keywwords. Exclude certification or license\")\n",
    "    soft_skills: List[str] = Field(default_factory=list, description=\"A list of soft skills mentioned in the resume. Soft skills are qualities like communication, teamwork, leadership. Keep it as keywwords. Exclude required languages\")\n",
    "    languages: List[str]= Field(default_factory=list, description=\"A list of language proficiencies mentioned in the resume. If the resume does not mention any languages, then fill this with the language that the resume is written in\")\n",
    "    experience: List[Experience] = Field(default_factory=list, description=\"A list of past work experiences in reverse chronological order (most recent first).\")\n",
    "    education: List[Education] = Field(default_factory=list, description=\"A list of educational qualifications\")\n",
    "    certifications: List[str] = Field(default_factory=list, description=\"A list of certifications or licenses related with hard skills, medical skills, and software tools mentioned in the resume. For example, AWS Certified Solutions Architect, PMP, etc. Certifications must exclude any work role IDs, only include valid licenses or certifications.\")\n",
    "\n",
    "# Create the parser\n",
    "resume_parser = PydanticOutputParser(pydantic_object=Resume)\n",
    "format_instructions = resume_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd7233d",
   "metadata": {},
   "source": [
    "## Job Desc Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f413bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models for job desc\n",
    "\n",
    "class JD(BaseModel):\n",
    "    company_name: Optional[str] = Field(None, description=\"Name of the company posting the job\")\n",
    "    role_title: Optional[str] = Field(None, description=\"Job title or position being offered\")\n",
    "    application_deadline: Optional[str] = Field(None, description=\"The deadline for submitting applications. Dates must be in ISO 8601 format (YYYY-MM-DDTHH:MM:SS)\")\n",
    "    date_posted: Optional[str] = Field(None, description=\"The date when the job was posted. Dates must be in ISO 8601 format (YYYY-MM-DDTHH:MM:SS)\")\n",
    "    employment_type: Optional[str] = Field(None, description=\"Type of employment, such as Full-time, Part-time, Contract, Freelance, or Internship. If not stated, it is assumed to be Full-time\")\n",
    "    about_the_company: Optional[str] = Field(None, description=\"A brief overview or description of the company\")\n",
    "    job_responsibilities: List[str] = Field(default_factory=list, description=\"A list of key duties, tasks, or responsibilities associated with the job\")\n",
    "    required_hard_skills: List[str] = Field(default_factory=list, description=\"A list of technical or hard skills required or preferred for the job. Keep it as keywords. This includes programming languages, software tools, or frameworks like Python, Java, SQL\")\n",
    "    required_soft_skills: List[str] = Field(default_factory=list, description=\"A list of soft skills or character required or preferred for the job. Keep it as keywords. This includes communication, teamwork, or leadership skills\")   \n",
    "    required_language_proficiencies: List[str] = Field(default_factory=list, description=\"A list of language proficiencies required for the job if stated. If the job description does not mention any languages, then fill this with the language that the job description is written in\")\n",
    "    required_education: Optional[str] = Field(None, description=\"The minimum educational qualification required for the job, such as a degree or certification\")\n",
    "    required_work_authorization: Optional[str] = Field(None, description=\"Work authorization required for the job\")\n",
    "    job_location: Optional[str] = Field(None, description=\"Location where the job is based, such as a city or remote\")\n",
    "    certifications: List[str] = Field(default_factory=list, description=\"A list of certifications or licenses related with hard skills, medical skills, and software tools mentioned in the resume. certifications should relate only to verifiable credentials (e.g., AWS, CISSP, PMP). Do not include work roles or job titles as certifications\")\n",
    "    \n",
    "# Create the parser\n",
    "jd_parser = PydanticOutputParser(pydantic_object=JD)\n",
    "format_instructions = jd_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2cac6",
   "metadata": {},
   "source": [
    "## Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb440d",
   "metadata": {},
   "source": [
    "### Parse resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8aa5a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ministral-3b-2410\n",
      "ministral-3b-latest\n",
      "ministral-8b-2410\n",
      "ministral-8b-latest\n",
      "open-mistral-7b\n",
      "mistral-tiny\n",
      "mistral-tiny-2312\n",
      "open-mistral-nemo\n",
      "open-mistral-nemo-2407\n",
      "mistral-tiny-2407\n",
      "mistral-tiny-latest\n",
      "open-mixtral-8x7b\n",
      "mistral-small\n",
      "mistral-small-2312\n",
      "open-mixtral-8x22b\n",
      "open-mixtral-8x22b-2404\n",
      "mistral-small-2402\n",
      "mistral-small-2409\n",
      "mistral-medium-2312\n",
      "mistral-large-2402\n",
      "mistral-large-2407\n",
      "mistral-large-2411\n",
      "mistral-large-latest\n",
      "pixtral-large-2411\n",
      "pixtral-large-latest\n",
      "mistral-large-pixtral-2411\n",
      "codestral-2405\n",
      "codestral-2501\n",
      "codestral-latest\n",
      "codestral-2412\n",
      "codestral-2411-rc5\n",
      "devstral-small-2505\n",
      "devstral-small-latest\n",
      "pixtral-12b-2409\n",
      "pixtral-12b\n",
      "pixtral-12b-latest\n",
      "mistral-small-2501\n",
      "mistral-small-2503\n",
      "mistral-small-latest\n",
      "mistral-saba-2502\n",
      "mistral-saba-latest\n",
      "mistral-medium-2505\n",
      "mistral-medium-latest\n",
      "mistral-medium\n",
      "mistral-embed\n",
      "codestral-embed\n",
      "codestral-embed-2505\n",
      "mistral-moderation-2411\n",
      "mistral-moderation-latest\n",
      "mistral-ocr-2503\n",
      "mistral-ocr-2505\n",
      "mistral-ocr-latest\n"
     ]
    }
   ],
   "source": [
    "# model options\n",
    "\n",
    "models = mistral.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4614ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_with_mistral(text: str, parser, format_instructions: str, label: str) -> BaseModel:\n",
    "\n",
    "    prompt = (\n",
    "    f\"Parse the following text into a structured format according to the provided schema.\"\n",
    "    f\"If the same role at the same company appears more than once, merge the role descriptions and preserve the earliest start and latest end dates.\"\n",
    "    f\"{format_instructions}\\n\\n\"\n",
    "    f\"{label}:\\n{text}\"\n",
    ")\n",
    "\n",
    "    response = mistral.chat.complete(\n",
    "        model=\"mistral-medium-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "    raw = response.choices[0].message.content\n",
    "    return parser.parse(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9e7d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = df[\"resume_text\"].iloc[6236]\n",
    "parsed_resume = parse_with_mistral(resume_text, resume_parser, resume_parser.get_format_instructions(), \"Resume\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "73d61c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": null,\n",
      "  \"location_preference\": null,\n",
      "  \"work_authorizaton\": null,\n",
      "  \"employment_type_preference\": null,\n",
      "  \"hard_skills\": [\n",
      "    \"Microsoft Excel\",\n",
      "    \"Microsoft Outlook\",\n",
      "    \"Adobe Software\",\n",
      "    \"Microsoft Office Suite\",\n",
      "    \"Microsoft Access\",\n",
      "    \"Database Management\",\n",
      "    \"Data Compilation\",\n",
      "    \"Data Review\",\n",
      "    \"Data Verification\"\n",
      "  ],\n",
      "  \"soft_skills\": [\n",
      "    \"Decision Making\",\n",
      "    \"Service-Oriented\",\n",
      "    \"Self-Starter\",\n",
      "    \"Workflow Management\",\n",
      "    \"Attention to Detail\",\n",
      "    \"Multitasking and Prioritization\",\n",
      "    \"Time Management\",\n",
      "    \"Team Player\",\n",
      "    \"Communication\",\n",
      "    \"Leadership\"\n",
      "  ],\n",
      "  \"languages\": [\n",
      "    \"English\"\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"role\": \"Data Entry Specialist\",\n",
      "      \"company\": \"Sonic Healthcare Usa\",\n",
      "      \"date_start\": \"2020-09-01T00:00:00\",\n",
      "      \"date_end\": \"current\",\n",
      "      \"role_description\": \"Input client information into spreadsheets and company database to provide leaders with quick access to essential client data. Identified, corrected and reported data entry errors. Completed accurate and efficient data entry and database updates to support business operations. Identified and corrected data entry errors to prevent duplication across systems. Compiled data from source documents prior to data entry. Reviewed and updated account information in company computer system. Identified errors in data entry and related issues by mentioning to supervisors for resolution. Sorted source documents and organized to be filed. Adhered to strict data confidentiality policies to prevent information leakage. Communicated with coworkers regarding deadlines and project milestones. Proofread and edited documents to correct errors. Documented data entry completions in corresponding logbooks. Executed data verification to ensure expedient error detection. Exceeded quality goals to support team productivity. Monitored updates to company databases and corrected identified errors. Transferred completed work to Title Officers for review and approval. Reviewed source documents to locate required data for entry. Produced new orders in Streamline Management Services and Greenfolders to manage samples and associated data.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Manager\",\n",
      "      \"company\": \"Community Health System\",\n",
      "      \"date_start\": \"2018-11-01T00:00:00\",\n",
      "      \"date_end\": \"2020-09-01T00:00:00\",\n",
      "      \"role_description\": \"Trained employees on additional job positions to maintain coverage of roles. Assigned tasks to associates to fit skill levels and maximize team performance. Greeted and encouraged feedback from customers to implement in-store operational changes. Completed thorough opening, closing and shift change functions to maintain operational standards each day. Enforced customer service standards and resolved customer problems to uphold quality service. Exercised good judgment and decision-making in escalating concerns and resolving issues. Oversaw daily workloads and workflow for smooth operations. Managed shifts in absence of store manager to deliver excellent customer service while promoting sales.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Crew Member\",\n",
      "      \"company\": \"Marcus Corporation\",\n",
      "      \"date_start\": \"2016-06-01T00:00:00\",\n",
      "      \"date_end\": \"2018-08-01T00:00:00\",\n",
      "      \"role_description\": \"Wiped down tables and equipment, swept and refilled stock. Kept restaurant lobby, front counter and restrooms neat and clean throughout shift. Packed fast food products in approved containers, cups and bags. Entered orders into computer system to send order details to kitchen, mentioning customers' special requests and food allergies in person. Prepared quality products while maintaining portion control and presentation within service goal times. Drove team success by quickly completing assigned tasks. Demonstrated proper food safety practices by accurately completing quality control checklist. Organized and restocked supplies to support operations and team productivity. Served food quickly for positive guest experiences. Packaged menu items into bags or trays and placed drink orders into carriers. Explained current promotional information and items to patrons. Assisted management with inventory control and stock ordering. Presented orders to guests within anticipated service times. Restocked supplies, removed trash and cleaned areas. Answered customer questions and took orders. Prepared products by adding tags and readying pallets for restocking. Operated fryers and grills, assisted with putting orders together and bagged items for customers. Totaled bills, accepted payments and returned change.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Cashier\",\n",
      "      \"company\": \"Kohler\",\n",
      "      \"date_start\": \"2017-06-01T00:00:00\",\n",
      "      \"date_end\": \"2018-04-01T00:00:00\",\n",
      "      \"role_description\": \"Collected payments and provided accurate change. Completed daily recovery tasks to keep areas clean and neat for maximum productivity. Worked closely with front-end staff to assist customers. Accepted cash and credit card payments, issued receipts and provided change. Trained new team members in cash register operation, stock procedures and customer services. Learned roles of other departments to provide coverage and keep store operational. Operated cash register or POS system to receive payment by cash, check and credit card. Reported pricing discrepancies to supervisor. Answered customer questions and provided store information. Delivered high level of customer service to patrons using active listening and engagement skills. Preserved appearance of store by arranging and replenishing displays and merchandise racks.\"\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"High School Diploma\",\n",
      "      \"institution\": \"Idea Homeschool\",\n",
      "      \"date_start\": null,\n",
      "      \"date_end\": null,\n",
      "      \"grade\": null,\n",
      "      \"description\": null\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(parsed_resume.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143dea9a",
   "metadata": {},
   "source": [
    "### Parse job desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c1b7e767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "Hope you are doing great today. Please find the job description below. Let me know your job interest as soon as possible. I will highly appreciate it if you can refer somebody suitable for this position. \n",
      "Role: Data Engineer (Oracle and DataStage).Location: RemoteContract Position\n",
      "Job Description:RoleResponsibilities:Skills: Oracle, Datastage, UNIX, PLSQL, SQL. Good to have: AWS, Matillion, Snowflake. Data engineering experience; expert level experience with SQL. Experience with the cloud (AWS, Azure andor Google Cloud Platform).  Experience in cloud-based data warehouses (Snowflake, Google BigQuery, Amazon Redshift, Azure Synapse Analytics).  Experience with cloud-based ETLELT tools (Matillion, Glue, Data Factory) and data modelling.  Experience with version control systems (Git, SVN).  Understanding of and willingness to embrace Agile Principles. \n",
      "Looking forward to your response . \n",
      "Shubhanshu Tripathishubhanshu.t@cblsolutions.com 469-947-7816 (Ext  209)Cerebral Technologies, Inc (D.B.A CBLSolutions) http:cblsolutions.com400 E Royal Lane, Ste 235, Irving, TX - 75039 Linkedin: https:www.linkedin.cominshubhanshu-tripathi-058228213 \n"
     ]
    }
   ],
   "source": [
    "print(df['job_description_text'].iloc[6236])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57dd896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_jd = parse_with_mistral(df['job_description_text'].iloc[6236], jd_parser, jd_parser.get_format_instructions(), \"Job Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3fefe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"company_name\": \"Cerebral Technologies, Inc (D.B.A CBLSolutions)\",\n",
      "  \"role_title\": \"Data Engineer (Oracle and DataStage)\",\n",
      "  \"application_deadline\": null,\n",
      "  \"date_posted\": null,\n",
      "  \"employment_type\": \"Contract\",\n",
      "  \"about_the_company\": null,\n",
      "  \"job_responsibilities\": [\n",
      "    \"Data engineering experience\",\n",
      "    \"Expert level experience with SQL\",\n",
      "    \"Experience with the cloud (AWS, Azure, and/or Google Cloud Platform)\",\n",
      "    \"Experience in cloud-based data warehouses (Snowflake, Google BigQuery, Amazon Redshift, Azure Synapse Analytics)\",\n",
      "    \"Experience with cloud-based ETL/ELT tools (Matillion, Glue, Data Factory) and data modeling\",\n",
      "    \"Experience with version control systems (Git, SVN)\",\n",
      "    \"Understanding of and willingness to embrace Agile Principles\"\n",
      "  ],\n",
      "  \"required_hard_skills\": [\n",
      "    \"Oracle\",\n",
      "    \"DataStage\",\n",
      "    \"UNIX\",\n",
      "    \"PLSQL\",\n",
      "    \"SQL\",\n",
      "    \"AWS\",\n",
      "    \"Matillion\",\n",
      "    \"Snowflake\",\n",
      "    \"Google Cloud Platform\",\n",
      "    \"Azure\",\n",
      "    \"Google BigQuery\",\n",
      "    \"Amazon Redshift\",\n",
      "    \"Azure Synapse Analytics\",\n",
      "    \"Glue\",\n",
      "    \"Data Factory\",\n",
      "    \"Git\",\n",
      "    \"SVN\"\n",
      "  ],\n",
      "  \"required_soft_skills\": [\n",
      "    \"Agile Principles\"\n",
      "  ],\n",
      "  \"required_language_proficiencies\": [\n",
      "    \"English\"\n",
      "  ],\n",
      "  \"required_education\": null,\n",
      "  \"required_work_authorization\": null,\n",
      "  \"job_location\": \"Remote\",\n",
      "  \"certifications\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(parsed_jd.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b71f4",
   "metadata": {},
   "source": [
    "### Parse 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5fbce08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>job_description_text</th>\n",
       "      <th>label</th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>resume_id</th>\n",
       "      <th>job_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SummaryHighly motivated Sales Associate with e...</td>\n",
       "      <td>Net2Source Inc. is an award-winning total work...</td>\n",
       "      <td>No Fit</td>\n",
       "      <td>2024-10-10</td>\n",
       "      <td>RES_QDvgj241</td>\n",
       "      <td>JD_QDvgj241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Professional SummaryCurrently working with Cat...</td>\n",
       "      <td>At Salas OBrien we tell our clients that were ...</td>\n",
       "      <td>No Fit</td>\n",
       "      <td>2024-06-09</td>\n",
       "      <td>RES_tvKW28PW</td>\n",
       "      <td>JD_tvKW28PW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SummaryI started my construction career in Jun...</td>\n",
       "      <td>Schweitzer Engineering Laboratories (SEL) Infr...</td>\n",
       "      <td>No Fit</td>\n",
       "      <td>2024-11-10</td>\n",
       "      <td>RES_Pg6ipOr5</td>\n",
       "      <td>JD_Pg6ipOr5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SummaryCertified Electrical Foremanwith thirte...</td>\n",
       "      <td>Mizick Miller &amp; Company, Inc. is looking for a...</td>\n",
       "      <td>No Fit</td>\n",
       "      <td>2024-09-12</td>\n",
       "      <td>RES_O5bebNRA</td>\n",
       "      <td>JD_O5bebNRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SummaryWith extensive experience in business/r...</td>\n",
       "      <td>Life at Capgemini\\nCapgemini supports all aspe...</td>\n",
       "      <td>No Fit</td>\n",
       "      <td>2024-02-04</td>\n",
       "      <td>RES_JWSvWYY5</td>\n",
       "      <td>JD_JWSvWYY5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>SummaryResults-driven Data Entry Clerk with ex...</td>\n",
       "      <td>Hi,\\nHope you are doing great today. Please fi...</td>\n",
       "      <td>Good Fit</td>\n",
       "      <td>2024-06-02</td>\n",
       "      <td>RES_vNEJ62Py</td>\n",
       "      <td>JD_vNEJ62Py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6237</th>\n",
       "      <td>Professional SummaryWith the attitude of learn...</td>\n",
       "      <td>Job Title: DHT - Front End Software Engineer W...</td>\n",
       "      <td>Good Fit</td>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>RES_DPqh0lVb</td>\n",
       "      <td>JD_DPqh0lVb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6238</th>\n",
       "      <td>Summary•        \\nOver\\nThree years of extensi...</td>\n",
       "      <td>LHH Recruitment Solutions is looking for a Sof...</td>\n",
       "      <td>Good Fit</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>RES_1HWrRA5T</td>\n",
       "      <td>JD_1HWrRA5T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>ProfileAbility to prioritize and multi-task in...</td>\n",
       "      <td>Our client is a growing Medical Device company...</td>\n",
       "      <td>Good Fit</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>RES_XdUNowSD</td>\n",
       "      <td>JD_XdUNowSD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6240</th>\n",
       "      <td>SummaryFull stack Software Engineer with 8+ ye...</td>\n",
       "      <td>Robert Half is looking for a Senior Full Stack...</td>\n",
       "      <td>Good Fit</td>\n",
       "      <td>2024-08-22</td>\n",
       "      <td>RES_2RPwzELC</td>\n",
       "      <td>JD_2RPwzELC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            resume_text  \\\n",
       "0     SummaryHighly motivated Sales Associate with e...   \n",
       "1     Professional SummaryCurrently working with Cat...   \n",
       "2     SummaryI started my construction career in Jun...   \n",
       "3     SummaryCertified Electrical Foremanwith thirte...   \n",
       "4     SummaryWith extensive experience in business/r...   \n",
       "6236  SummaryResults-driven Data Entry Clerk with ex...   \n",
       "6237  Professional SummaryWith the attitude of learn...   \n",
       "6238  Summary•        \\nOver\\nThree years of extensi...   \n",
       "6239  ProfileAbility to prioritize and multi-task in...   \n",
       "6240  SummaryFull stack Software Engineer with 8+ ye...   \n",
       "\n",
       "                                   job_description_text     label  \\\n",
       "0     Net2Source Inc. is an award-winning total work...    No Fit   \n",
       "1     At Salas OBrien we tell our clients that were ...    No Fit   \n",
       "2     Schweitzer Engineering Laboratories (SEL) Infr...    No Fit   \n",
       "3     Mizick Miller & Company, Inc. is looking for a...    No Fit   \n",
       "4     Life at Capgemini\\nCapgemini supports all aspe...    No Fit   \n",
       "6236  Hi,\\nHope you are doing great today. Please fi...  Good Fit   \n",
       "6237  Job Title: DHT - Front End Software Engineer W...  Good Fit   \n",
       "6238  LHH Recruitment Solutions is looking for a Sof...  Good Fit   \n",
       "6239  Our client is a growing Medical Device company...  Good Fit   \n",
       "6240  Robert Half is looking for a Senior Full Stack...  Good Fit   \n",
       "\n",
       "     snapshot_date     resume_id       job_id  \n",
       "0       2024-10-10  RES_QDvgj241  JD_QDvgj241  \n",
       "1       2024-06-09  RES_tvKW28PW  JD_tvKW28PW  \n",
       "2       2024-11-10  RES_Pg6ipOr5  JD_Pg6ipOr5  \n",
       "3       2024-09-12  RES_O5bebNRA  JD_O5bebNRA  \n",
       "4       2024-02-04  RES_JWSvWYY5  JD_JWSvWYY5  \n",
       "6236    2024-06-02  RES_vNEJ62Py  JD_vNEJ62Py  \n",
       "6237    2024-09-01  RES_DPqh0lVb  JD_DPqh0lVb  \n",
       "6238    2024-11-02  RES_1HWrRA5T  JD_1HWrRA5T  \n",
       "6239    2024-07-26  RES_XdUNowSD  JD_XdUNowSD  \n",
       "6240    2024-08-22  RES_2RPwzELC  JD_2RPwzELC  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset = pd.concat([df[:5], df[-5:]])\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b24e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:01<00:00, 18.14s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(df_subset.iterrows(), total=len(df_subset)):\n",
    "    resume_text = row['resume_text']\n",
    "    jd_text = row['job_description_text']\n",
    "    \n",
    "    try:\n",
    "        # Process resume\n",
    "        parsed_resume = parse_with_mistral(\n",
    "            resume_text,\n",
    "            resume_parser,\n",
    "            resume_parser.get_format_instructions(),\n",
    "            \"Resume\"\n",
    "        )\n",
    "        parsed_resume_dict = parsed_resume.model_dump(mode=\"json\")\n",
    "        resume_output_path = os.path.join('examples_mistral', 'resume', f\"{idx}.json\")\n",
    "        os.makedirs(os.path.dirname(resume_output_path), exist_ok=True) \n",
    "        with open(resume_output_path, \"w\") as f:\n",
    "            json.dump(parsed_resume_dict, f, indent=2)\n",
    "\n",
    "        # Process JD\n",
    "        parsed_jd = parse_with_mistral(\n",
    "            jd_text,\n",
    "            jd_parser,\n",
    "            jd_parser.get_format_instructions(),\n",
    "            \"Job Description\"\n",
    "        )\n",
    "        parsed_jd_dict = parsed_jd.model_dump(mode=\"json\")\n",
    "        jd_output_path = os.path.join('examples_mistral', 'jd', f\"{idx}.json\")\n",
    "        os.makedirs(os.path.dirname(jd_output_path), exist_ok=True) \n",
    "        with open(jd_output_path, \"w\") as f:\n",
    "            json.dump(parsed_jd_dict, f, indent=2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing row {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051f325",
   "metadata": {},
   "source": [
    "# Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9271ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "uri = os.environ.get(\"MONGO_DB_URL\")\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1472c4d3-244d-4412-a25b-300451e8b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDBIntegration\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", os.environ.get(\"MONGO_DB_URL\")) \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", os.environ.get(\"MONGO_DB_URL\")) \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac06e8-c7b8-48c1-85d0-12d47269d816",
   "metadata": {},
   "source": [
    "### Save in mongodb per item (incrementing table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e17358c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/6241 [03:16<23:37:10, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing row 11: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 590/6241 [3:03:25<29:16:54, 18.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m resume_collection.insert_one(parsed_resume_dict) \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Process JD\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m parsed_jd = \u001b[43mparse_with_mistral\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjd_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjd_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjd_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_format_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mJob Description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m parsed_jd_dict = parsed_jd.model_dump(mode=\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m parsed_jd_dict[\u001b[33m\"\u001b[39m\u001b[33mrow_idx\u001b[39m\u001b[33m\"\u001b[39m] = idx  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mparse_with_mistral\u001b[39m\u001b[34m(text, parser, format_instructions, label)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_with_mistral\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, parser, format_instructions: \u001b[38;5;28mstr\u001b[39m, label: \u001b[38;5;28mstr\u001b[39m) -> BaseModel:\n\u001b[32m      5\u001b[39m     prompt = (\n\u001b[32m      6\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParse the following text into a structured format according to the provided schema.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf the same role at the same company appears more than once, merge the role descriptions and preserve the earliest start and latest end dates.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_instructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     response = \u001b[43mmistral\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistral-medium-latest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     raw = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser.parse(raw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mistralai/chat.py:219\u001b[39m, in \u001b[36mChat.complete\u001b[39m\u001b[34m(self, model, messages, temperature, top_p, max_tokens, stream, stop, random_seed, response_format, tools, tool_choice, presence_penalty, frequency_penalty, n, prediction, parallel_tool_calls, safe_prompt, retries, server_url, timeout_ms, http_headers)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(retries, utils.RetryConfig):\n\u001b[32m    217\u001b[39m     retry_config = (retries, [\u001b[33m\"\u001b[39m\u001b[33m429\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m500\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m502\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m503\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m504\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m http_res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhook_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHookContext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat_completion_v1_chat_completions_post\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43moauth2_scopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43msecurity_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_security_from_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msdk_configuration\u001b[49m\u001b[43m.\u001b[49m\u001b[43msecurity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSecurity\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m422\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4XX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m5XX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m response_data: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m utils.match_response(http_res, \u001b[33m\"\u001b[39m\u001b[33m200\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mistralai/basesdk.py:277\u001b[39m, in \u001b[36mBaseSDK.do_request\u001b[39m\u001b[34m(self, hook_ctx, request, error_status_codes, stream, retry_config)\u001b[39m\n\u001b[32m    275\u001b[39m     http_res = utils.retry(do, utils.Retries(retry_config[\u001b[32m0\u001b[39m], retry_config[\u001b[32m1\u001b[39m]))\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     http_res = \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m utils.match_status_codes(error_status_codes, http_res.status_code):\n\u001b[32m    280\u001b[39m     http_res = \u001b[38;5;28mself\u001b[39m.sdk_configuration.get_hooks().after_success(\n\u001b[32m    281\u001b[39m         AfterSuccessContext(hook_ctx), http_res\n\u001b[32m    282\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mistralai/basesdk.py:238\u001b[39m, in \u001b[36mBaseSDK.do_request.<locals>.do\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    236\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mclient is required\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     http_res = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    240\u001b[39m     _, e = \u001b[38;5;28mself\u001b[39m.sdk_configuration.get_hooks().after_error(\n\u001b[32m    241\u001b[39m         AfterErrorContext(hook_ctx), \u001b[38;5;28;01mNone\u001b[39;00m, e\n\u001b[32m    242\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "db = client[\"jobmirror_db\"]\n",
    "resume_collection = db[\"resumes\"]\n",
    "jd_collection = db[\"job_descriptions\"]\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    resume_text = row['resume_text']\n",
    "    jd_text = row['job_description_text']\n",
    "    try:\n",
    "        # Process resume\n",
    "        parsed_resume = parse_with_mistral(\n",
    "            resume_text,\n",
    "            resume_parser,\n",
    "            resume_parser.get_format_instructions(),\n",
    "            \"Resume\"\n",
    "        )\n",
    "        parsed_resume_dict = parsed_resume.model_dump(mode=\"json\")\n",
    "        parsed_resume_dict[\"row_idx\"] = idx  \n",
    "        resume_collection.insert_one(parsed_resume_dict) \n",
    "\n",
    "        # Process JD\n",
    "        parsed_jd = parse_with_mistral(\n",
    "            jd_text,\n",
    "            jd_parser,\n",
    "            jd_parser.get_format_instructions(),\n",
    "            \"Job Description\"\n",
    "        )\n",
    "        parsed_jd_dict = parsed_jd.model_dump(mode=\"json\")\n",
    "        parsed_jd_dict[\"row_idx\"] = idx  \n",
    "        jd_collection.insert_one(parsed_jd_dict)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing row {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2db48477-91ec-4fda-bc2a-782b1dd6f05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 640, 'electionId': ObjectId('7fffffff00000000000001b7'), 'opTime': {'ts': Timestamp(1748846844, 37), 't': 439}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1748846844, 37), 'signature': {'hash': b',;\\x0f\\x14\\xdc\\x04\\xae#\\x05\\xf8\\x8d\\xce\\n\\x97y\\xad\\xfd\\xffO\\xa5', 'keyId': 7450535577176244240}}, 'operationTime': Timestamp(1748846844, 37)}, acknowledged=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear collections\n",
    "\n",
    "db = client[\"jobmirror_db\"]\n",
    "resume_collection = db[\"resumes\"]\n",
    "jd_collection = db[\"job_descriptions\"]\n",
    "\n",
    "resume_collection.delete_many({})\n",
    "jd_collection.delete_many({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180aa45",
   "metadata": {},
   "source": [
    "Convert into PySpark Dataframe (Overwrite table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "396cb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_type_to_spark_type(annotation):\n",
    "    origin = get_origin(annotation)\n",
    "\n",
    "    if origin is Union:  # Handle Optional\n",
    "        args = [arg for arg in get_args(annotation) if arg is not type(None)]\n",
    "        return python_type_to_spark_type(args[0])\n",
    "\n",
    "    if origin in (list, List):\n",
    "        element_type = python_type_to_spark_type(get_args(annotation)[0])\n",
    "        return ArrayType(element_type)\n",
    "\n",
    "    if isinstance(annotation, type):\n",
    "        if issubclass(annotation, BaseModel):\n",
    "            return pydantic_to_spark_schema(annotation)\n",
    "        if issubclass(annotation, str):\n",
    "            return StringType()\n",
    "        if issubclass(annotation, int):\n",
    "            return IntegerType()\n",
    "        if issubclass(annotation, float):\n",
    "            return FloatType()\n",
    "        if issubclass(annotation, bool):\n",
    "            return BooleanType()\n",
    "        if issubclass(annotation, datetime.datetime):\n",
    "            return StringType()\n",
    "\n",
    "    return StringType()\n",
    "\n",
    "def pydantic_to_spark_schema(model: type) -> StructType:\n",
    "    fields = []\n",
    "\n",
    "    for name, field in model.model_fields.items():\n",
    "        annotation = field.annotation\n",
    "\n",
    "        spark_type = python_type_to_spark_type(annotation)\n",
    "        fields.append(StructField(name, spark_type, True))  # assume all nullable\n",
    "    fields.append(StructField('snapshot_date', StringType(), True))\n",
    "    fields.append(StructField('id', StringType(), True))\n",
    "\n",
    "    return StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efbe69-80ca-4f8c-af7d-b5a9c532980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 159/6241 [1:32:54<658:56:45, 390.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing row 158: Server disconnected without sending a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 182/6241 [1:48:55<50:55:45, 30.26s/it]  "
     ]
    }
   ],
   "source": [
    "parsed_resumes = []\n",
    "parsed_jds = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        # Parse resume\n",
    "        parsed_resume = parse_with_mistral(\n",
    "            row['resume_text'],\n",
    "            resume_parser,\n",
    "            resume_parser.get_format_instructions(),\n",
    "            \"Resume\"\n",
    "        )\n",
    "        parsed_resume_dict = parsed_resume.model_dump(mode=\"json\")\n",
    "        parsed_resume_dict['snapshot_date'] = row['snapshot_date']\n",
    "        parsed_resume_dict['id'] = row['resume_id']\n",
    "        parsed_resumes.append(parsed_resume_dict)\n",
    "\n",
    "        # Parse JD\n",
    "        parsed_jd = parse_with_mistral(\n",
    "            row['job_description_text'],\n",
    "            jd_parser,\n",
    "            jd_parser.get_format_instructions(),\n",
    "            \"Job Description\"\n",
    "        )\n",
    "        parsed_jd_dict = parsed_jd.model_dump(mode=\"json\")\n",
    "        parsed_jd_dict['snapshot_date'] = row['snapshot_date']\n",
    "        parsed_jd_dict['id'] = row['job_id']\n",
    "        parsed_jds.append(parsed_jd_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing row {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_schema = pydantic_to_spark_schema(Resume)\n",
    "jd_schema = pydantic_to_spark_schema(JD)\n",
    "\n",
    "resume_df = spark.createDataFrame(parsed_resumes, schema=resume_schema)\n",
    "jd_df = spark.createDataFrame(parsed_jds, schema=jd_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed4b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df.write.format(\"mongodb\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"database\", \"jobmirror\") \\\n",
    "            .option(\"collection\", \"resumes\") \\\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_df.write.format(\"mongodb\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"database\", \"jobmirror\") \\\n",
    "            .option(\"collection\", \"jd\") \\\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f185d77",
   "metadata": {},
   "source": [
    "# SILVER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a8a66",
   "metadata": {},
   "source": [
    "# GOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5416e",
   "metadata": {},
   "source": [
    "## Get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52d6567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\", task_type=\"SEMANTIC_SIMILARITY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f948ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_required_skills = embedding_model.embed_documents(parsed_jd.required_hard_skills)\n",
    "embeddings_skills_owned = embedding_model.embed_documents(parsed_resume.hard_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93af6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_skills = np.array(embeddings_required_skills)\n",
    "skills_owned = np.array(embeddings_skills_owned)\n",
    "\n",
    "# Normalize embeddings to unit vectors (L2 norm)\n",
    "required_skills = required_skills / np.linalg.norm(required_skills, axis=1, keepdims=True)\n",
    "skills_owned = skills_owned / np.linalg.norm(skills_owned, axis=1, keepdims=True)\n",
    "\n",
    "# Compute cosine similarity matrix by dot product\n",
    "similarity_matrix = np.dot(required_skills, skills_owned.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ae5fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required: PostgreSQL  <=> Best Owned: PostgreSQL  | Similarity: 1.00\n",
      "Required: Express  <=> Best Owned: EF  | Similarity: 0.63\n",
      "Required: React  <=> Best Owned: HTML5  | Similarity: 0.63\n",
      "Required: NodeJS  <=> Best Owned: AngularJS  | Similarity: 0.73\n",
      "Required: Redux  <=> Best Owned: Redmine  | Similarity: 0.63\n",
      "Required: HTML  <=> Best Owned: HTML  | Similarity: 1.00\n",
      "Required: CSS  <=> Best Owned: CSS  | Similarity: 1.00\n",
      "Required: JavaScript  <=> Best Owned: jQuery  | Similarity: 0.86\n",
      "Required: JSON  <=> Best Owned: JSON  | Similarity: 1.00\n",
      "Required: Git  <=> Best Owned: GIT  | Similarity: 0.95\n",
      "Required: REST  <=> Best Owned: REST  | Similarity: 1.00\n",
      "Required: Firebase  <=> Best Owned: Hangfire  | Similarity: 0.62\n",
      "Required: Material-UI  <=> Best Owned: AngularJS  | Similarity: 0.63\n",
      "Required: D3js  <=> Best Owned: jQuery  | Similarity: 0.72\n",
      "Required: Docker (Compose)  <=> Best Owned: Composer  | Similarity: 0.67\n",
      "Required: AWS  <=> Best Owned: AWS EC2  | Similarity: 0.85\n"
     ]
    }
   ],
   "source": [
    "best_matches = []\n",
    "\n",
    "for i, req_skill in enumerate(parsed_jd.required_hard_skills):\n",
    "    j = similarity_matrix[i].argmax()\n",
    "    score = similarity_matrix[i, j]\n",
    "    if score >= 0.6:\n",
    "        best_matches.append((req_skill, parsed_resume.hard_skills[j], score))\n",
    "\n",
    "# Print\n",
    "for req_skill, own_skill, score in best_matches:\n",
    "    print(f\"Required: {req_skill}  <=> Best Owned: {own_skill}  | Similarity: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "991dbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_role_name = embedding_model.embed_query(parsed_jd.role_title)\n",
    "embeddings_experience_titles = embedding_model.embed_documents([exp.role for exp in parsed_resume.experience])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65b9e94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senior Full Stack Engineer (PERN Stack)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_jd.role_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4e5d683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Software Developer',\n",
       " 'Software .Net Developer',\n",
       " 'Software Engineer and Professor']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[exp.role for exp in parsed_resume.experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31d693a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = np.array(embeddings_role_name)\n",
    "experiences = np.array(embeddings_experience_titles)\n",
    "\n",
    "# Normalize embeddings to unit vectors (L2 norm)\n",
    "role_name = role_name / np.linalg.norm(role_name)\n",
    "experiences = experiences / np.linalg.norm(experiences, axis=1, keepdims=True)\n",
    "\n",
    "# Compute cosine similarity matrix by dot product\n",
    "similarity_matrix = np.dot(experiences, role_name.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5a979df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65028087, 0.62905722, 0.6121288 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
