{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40458c5-7eee-4367-b41f-d751eba64a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9b973a-1f01-49d0-ae4b-fd403657f9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total entries: 6241\n",
      "Data columns (total 36 columns):\n",
      "#   Column                    Non-Null Count     Dtype          \n",
      "--- ------------------------- ------------------ ---------------\n",
      "0   resume_id                 6241               string         \n",
      "1   job_id                    6241               string         \n",
      "2   snapshot_date             6241               date           \n",
      "3   fit                       6241               string         \n",
      "4   company_name              3908               string         \n",
      "5   role_title                6046               string         \n",
      "6   about_the_company         3664               string         \n",
      "7   job_responsibilities      6241               array<string>  \n",
      "8   jd_soft_skills            6241               array<string>  \n",
      "9   required_language_proficiencies 6241               array<string>  \n",
      "10  job_snapshot              6241               date           \n",
      "11  jd_hard_skills_general    6240               array<string>  \n",
      "12  jd_hard_skills_specific   6240               array<string>  \n",
      "13  required_edu_level        4378               string         \n",
      "14  required_edu_field        4378               string         \n",
      "15  required_cert_categories  6241               array<string>  \n",
      "16  employment_type           6241               string         \n",
      "17  required_work_authorization 6241               string         \n",
      "18  job_location              6241               string         \n",
      "19  name                      237                string         \n",
      "20  soft_skills               6241               array<string>  \n",
      "21  languages                 6241               array<string>  \n",
      "22  experience                6241               array<struct<role:string,company:string,date_start:string,date_end:string,role_description:string,snapshot_date:string,id:string>>\n",
      "23  resume_snapshot           6241               string         \n",
      "24  hard_skills_general       6241               array<string>  \n",
      "25  hard_skills_specific      6241               array<string>  \n",
      "26  edu_highest_level         5779               string         \n",
      "27  edu_field                 5779               string         \n",
      "28  edu_gpa                   1324               float          \n",
      "29  edu_institution           5657               string         \n",
      "30  cert_categories           6241               array<string>  \n",
      "31  YoE_list                  6241               array<float>   \n",
      "32  employment_type_preference 6241               string         \n",
      "33  work_authorization        6241               string         \n",
      "34  location_preference       6241               string         \n",
      "35  exp_sim_list              6241               array<float>   \n",
      "\n",
      "\n",
      "+------------+-----------+-------------+------+-----------------+----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------+-------------------------------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+---------------------+------------------------+---------------+---------------------------+-------------+--------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+---------------------+-------+------------------------------------------+---------------+---------------------------------------------+--------------------------+------------------+-------------------+---------------------------------------------------------+\n",
      "|resume_id   |job_id     |snapshot_date|fit   |company_name     |role_title                                          |about_the_company                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |job_responsibilities                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |jd_soft_skills                                    |required_language_proficiencies|job_snapshot|jd_hard_skills_general                                                                                                                                                                                                                                                                                                             |jd_hard_skills_specific                                                                                                                                                                                                                                                                           |required_edu_level|required_edu_field   |required_cert_categories|employment_type|required_work_authorization|job_location |name          |soft_skills                                                                                                               |languages|experience                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |resume_snapshot|hard_skills_general                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |hard_skills_specific                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |edu_highest_level|edu_field            |edu_gpa|edu_institution                           |cert_categories|YoE_list                                     |employment_type_preference|work_authorization|location_preference|exp_sim_list                                             |\n",
      "+------------+-----------+-------------+------+-----------------+----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------+-------------------------------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+---------------------+------------------------+---------------+---------------------------+-------------+--------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+---------------------+-------+------------------------------------------+---------------+---------------------------------------------+--------------------------+------------------+-------------------+---------------------------------------------------------+\n",
      "|RES_s93wTCLp|JD_s93wTCLp|2021-07-15   |No Fit|NULL             |Electronic Engineer                                 |An acoustics, weapon launch and control systems, surveillance technology consulting firm.                                                                                                                                                                                                                                                                                                                                                                                                                                          |[Analyze and design analog and digital electronic circuits including analog signal conditioning, DC-DC power conversion, and communication interfaces, Integrate, test, troubleshoot, and verify electrical hardware, Develop test criteria and assist in the design of test systems, Perform component and architecture trade studies to determine optimal system solutions to design hardware and electronics based on commercial and industry components and technologies]                                                                                                                                                                                                                                                                                                                                                                                                                                                        |[]                                                |[English]                      |2021-07-15  |[human resources software]                                                                                                                                                                                                                                                                                                         |[humanic design human resources management system]                                                                                                                                                                                                                                                |NULL              |NULL                 |[]                      |full-time      |needed                     |VA           |Jessica Claire|[client, procurement, progress, supervising, supervision, phone]                                                          |[English]|[{Electrical & Instrumentation Engineer, Abbott Laboratories, 2001-01-01T00:00:00, 2001-01-01T00:00:00, Supervision of Installation of Electrical & Instrumentation equipment. Reviewed Engineering Drawings & Documents and ensured same comply with Project Specifications. Supervising the interfacing of instrumentation & telecom equipment with SCADA communications equipment. Experienced in procurement and expediting activities of various Instruments. Prepared MTO's (Material Take-off) for Instruments as per Project requirements for initiating procurement. Invited vendors in bidding process by sending RFQ's (Request for Quotation). Prepared reports for Expediting, Project progress and manufacturing & procurement status of Instruments. Monitored the overall Project schedule and ensured important Project milestones & deadlines are met., NULL, NULL}, {Technical Support Associate, IBM India Pvt. Ltd, 2001-01-01T00:00:00, NULL, Provided network infrastructure support to clients. Troubleshot issues pertaining to Outlook, Lotus Notes, VPN, Password Encryption, Hardware/Software, etc. Interacted with clients over the phone and computer to help resolve technical issues. Ensured follow-ups to close-out the unresolved tickets created for any technical issues. Coordinated with Level 2 support team for closure of escalated tickets., NULL, NULL}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |2021-07-15     |[object or component oriented development software, access software, lan software, operating system software, electronic mail software, analytical or scientific software, spreadsheet software, presentation software, word processing software, network security or virtual private network vpn management software, internet protocol ip multimedia subsystem software, switch or router software, industrial control software, wan switching software and firmware]                                                                               |[c++, cisco anyconnect, lan software, linux, ibm lotus notes, the mathworks matlab, microsoft excel, microsoft outlook, microsoft power point, microsoft windows, microsoft word, virtual private networking vpn software, open shortest path first ospf, python, enhanced interior gateway routing protocol eigrp, supervisory control and data acquisition scada software, cisco systems wan manager]                                                                                                                                                                                                                         |Master's Degree  |Engineering (General)|NULL   |Rochester Institute of Technology         |[]             |[0.0, 20.996578]                             |internship                |not needed        |Not specified      |[0.874028, 0.6370233]                                    |\n",
      "|RES_Z7yf1tu6|JD_Z7yf1tu6|2021-07-24   |No Fit|NULL             |Senior Manager, Data Architecture & Data Engineering|For over 50 years this non-profit organization has made it their mission to protect patients from unsafe and ineffective medical technologies and practices. More than 5,000 healthcare institutions and systems worldwide including four out of every five U.S. hospitals rely on the services of this organization to guide their operational and strategic decisions. In addition, they serve public and private payers, federal and state agencies, policymakers, ministries of health, associations, and accrediting agencies.|[Translate business strategic goals into actionable future-state data engineering and data science architectures and solutions., Delivery of full life-cycle Data Solutions from execution roadmaps, to implementation, and to operations. This includes a specific focus on ELT and ETL Data Pipeline solutions from inception to production., Lead end to end solution delivery of enterprise data warehouse, data marts, and data lake with governance., Drive the creation and maintenance of enterprise data and analytics capabilities, data solutions, and integrated data & analytics technology roadmaps that will guide our strategic investments and enterprise strategy., Deliver architecture connecting Data Science, Data Engineering, and Data Analytics practices together to deliver impactful data products., Strong partnership with DevOps, DataOps, business partners, engineering teams, and data management.]|[communication, teamwork, leadership]             |[English]                      |2021-07-24  |[development environment software, business intelligence and data analysis software, data base user interface and query software, metadata management software, data base management system software, web platform development software, analytical or scientific software, content workflow software, project management software]|[microsoft azure, apache spark, teradata active enterprise data warehouse, oracle master data management mdm suite, structured query language sql, nosql, javascript object notation json, google, national instruments ni vision builder for automated inspection ai, atlassian jira, confluence]|Bachelor's Degree |Computer Science & IT|[]                      |full-time      |not needed                 |Not specified|NULL          |[communication, teamwork, leadership, problem, thinking, work ethic, team, datum]                                         |[English]|[{Big Data Engineer, Cognizant Technology Solutions, NULL, NULL, Active Member of the chronicle team to build Data solutions to onboard metrics to Tursimo application. Responsible for designing, developing, scaling and maintaining redshift cluster to process 1200+ KPI metrics for Retail and 3p application. Designing, developing, scaling and maintaining EMR cluster to process hive scripts to parquet files to develop data model for the Query engine layer. Executed and managed high level, time critical projects to generate Free Cash Flow for the team. Developed Python Scripts to automate manual and repetitive engineering tasks. Developed Redshift clusters to implement SDPP policy for amazon Private Brands., NULL, NULL}, {Data warehouse Developer, 22Nd Century Technologies, 2012-06-01T00:00:00, 2016-09-01T00:00:00, Active Member of the Data Warehouse team for developing and maintaining ETL process for the Discovery Data warehouse. Active member of the ETL Team for developing a new EDW for the re-vamped bn.com. Designed and developed SSIS packages for the ETL processes through SQL Server 2013 Business Intelligence Development Studio. Responsible for designing, developing and maintaining ETL process to load the Aster Database a massively parallel (MPP) Analytic Platform. Maintained and enhanced Business Intelligence systems to convert secure and manage dataflow into the data warehouse systems. Expertise with Developing SQL scripts and Performance Tuning for New and existing Daily ETL process. Involved in maintaining health of the Database and scheduling scripts to send out alerts in case of any discrepancies. Involved in designing and implementing Shell scripts to archive historical click stream data to Hadoop Ecosystem for further analysis. Developed Scripts to move data from HDFS to RDBMS and vice-versa using SQOOP. Developed Scripts in analyzing and transforming Data with PIG and Hive. Evaluated ETL applications to support overall performance and improvement opportunities. Worked closely with the email marketing team in designing and developing the feeds at different granular levels for Email targeting campaigns. Expertise in Analyzing data Quality checks using shell scripts. Worked closely with the Business analyst and build data marts and summary tables to ease their access to the data warehouse. Responsible for gathering data requirements analysis, design, development, testing and implementation of data warehouse solutions. Implemented and validated automating feeds received from external party vendors. Automated the validation scripts of all exports that are generated out of the Data warehouse. Involved in all phases of Data mart creation of various business units across the company. Worked with various Aster SQL-MR functions to analyze clickstream and device and digital activity data and generated summary tables to analyze the KPI of the data available., NULL, NULL}, {Data Engineer/BI Developer, Amazon.com, 2010-01-01T00:00:00, 2012-05-01T00:00:00, Active Member of the Data warehouse team for building a new data warehouse on Amazon Framework, maintain existing data warehouse which is still current. Involved in all phases of the application development from business requirements analysis, design, development, and testing of the Data Warehouse. Wrote the design document for converting existing legacy data into new structures. Responsible in designing, monitoring, troubleshooting, debugging, testing job/workflow/dataflow to load data from multiple data sources for load into a custom reporting data mart. Responsible for modifying of all the three layers in the OBIEE RPD. Responsible for the development of custom reports and dashboards in OBIEE. Developed and delivered technical mapping documentations. Designed and executed Test plans to validate data and load validation. Responsible in Designing, validating and delivering Daily, Weekly and Quarterly reports from the new structures. Designed Unix and sql scripts to handle the daily feeds we receive and load them into tables. Responsible for data list processing for direct and email campaigns, process reporting, maintaining prospect databases, maintaining suppression data, ensuring process documentation and improvement, and providing data to other departments on request. Responsible for designing metadata, rpd and exposing business structures to various dashboards that were designed on OBIEE. Create periodic and ad-hoc reports using Transact SQL, SQL Stored Procedures, OBIEE, Perl, MS Excel, and MS Access. Took responsibility for discovery and documentation of processes to recover lost domain knowledge as a result of staff attrition., NULL, NULL}]|2021-07-24     |[data base user interface and query software, data base management system software, operating system software, object or component oriented development software, development environment software, customer relationship management crm software, enterprise resource planning erp software, clustering software, enterprise application integration software, document management software, metadata management software, business intelligence and data analysis software, analytical or scientific software]                                      |[structured query language sql, oracle pl/sql, transact-sql, unix, python, microsoft visual studio, eden sales manager, apache hadoop, apache pig, apache hive, amazon redshift, oracle e-business suite, aster data ncluster, oracle hyperion, oracle sql developer, microsoft sql server integration services ssis, apache sqoop, apache hbase, shell script, data warehousing software, data modeling software, adeptia etl suite, oracle business intelligence enterprise edition, data analysis software]                                                                                                                  |Master's Degree  |Computer Science & IT|3.33   |Syracuse University                       |[]             |[4.251882, 2.329911]                         |full-time                 |not needed        |Not specified      |[0.7299075, 0.6546017, 0.7022432]                        |\n",
      "|RES_hAp1XnJZ|JD_hAp1XnJZ|2021-07-21   |No Fit|DevCare Solutions|Business Analyst                                    |NULL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |[Conducting business process review and requirements elicitation and documentation, Facilitating executive and diverse groups to topic consensus on complex and competing perspectives, Providing strategic analysis and support to program, division and agency equivalent initiatives, Working with Agile Scrum, use of JIRA, user story development/validation, sprint planning and backlog grooming, Handling contract compliance and contract change notices, specific to requirements specifications, Practicing LEAN principles and supporting LPI initiatives, Working with programs governed by state and/or federal regulations]                                                                                                                                                                                                                                                                                           |[facilitation, analysis, consensus, communication]|[English]                      |2021-07-21  |[analytical or scientific software, content workflow software, graphical user interface development software, compliance software]                                                                                                                                                                                                 |[business analysis software, atlassian jira, bps compliance]                                                                                                                                                                                                                                      |NULL              |NULL                 |[]                      |contract       |needed                     |NY           |NULL          |[communication, strong analytical skills, excellent diagnostic skills, team]                                              |[English]|[{Data Analyst, Lockheed Martin Corporation, 2016-08-01T00:00:00, 2016-09-01T00:00:00, Worked with SAP removing duplicates. Worked with CRM interface to correct data. Analysis of customer data. Assisted with cleaning 442,000 customer account records. Extract Install Base data from the centralized repository and evaluate/prioritize refresh opportunities based. Developed and implemented complex Internet and Intranet applications on multiple platforms. Performed analysis and identified cost saving opportunities and potential program enhancements., NULL, NULL}, {Business Analyst, Halo Branded Solutions, 2015-11-01T00:00:00, 2016-07-01T00:00:00, Extracted Install Base data from the centralized repository and evaluate/prioritize refresh opportunities. Packaged prioritized customer data for inside/field sales consumption. Managed research and reporting of customer install base for BDM Sales team. Created metrics associated with funnel build and conversion rates and value. General field and inside sales support. Daily use of SAP to pull serial data to create client reports. Daily use of Sales Force to create and maintain client account information. Excel used daily to analyse and create client reports. Microsoft Server used in conjunction with Access to pull large data sets for analytical reporting., NULL, NULL}, {Data Analyst, Lockheed Martin Corporation, 2009-02-01T00:00:00, 2015-07-01T00:00:00, Excel and Access reports creation. Acted as a question/answer source for client. Maintained client Database. Identified process inefficiencies through gap analysis. Performed monthly data cleanup of all Carrier invoices input into system. Recommended operational improvements based on tracking and analysis. Performed analysis and identified cost saving opportunities and potential program enhancements., NULL, NULL}, {Database Administrator, Lockheed Martin Corporation, 2006-01-01T00:00:00, 2009-01-01T00:00:00, Acted as a question/answer source for client. Was responsible for monthly analysis of data input. Analyzed and made recommendations on data when necessary. Was responsible for monthly metrics reports. Was responsible for process improvements. Was responsible for root cause and corrective action analysis., NULL, NULL}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2021-07-21     |[data base management system software, enterprise resource planning erp software, customer relationship management crm software, spreadsheet software, data base user interface and query software, computer aided design cad software]                                                                                                                                                                                                                                                                                                               |[database design software, technology group international enterprise 21 erp, sap, salesforce.com salesforce crm, microsoft excel, microsoft access, microsoft sql server, sales force automation software, autodesk autocad, bentley geopak civil engineering suite]                                                                                                                                                                                                                                                                                                                                                            |Master's Degree  |Education & Training |NULL   |New York Institute of Technology          |[]             |[0.08487337, 0.66529775, 6.409309, 3.0006845]|full-time                 |not needed        |Not specified      |[0.8271955, 1.0000001, 0.8271955, 0.5974667]             |\n",
      "|RES_zsgGxd2s|JD_zsgGxd2s|2021-07-30   |No Fit|DevCare Solutions|Business Analyst                                    |NULL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |[Minimum of 12 years of general BA experience, Minimum of 7 years conducting business process review and requirements elicitation and documentation, Minimum of 5 years facilitating executive and diverse groups to topic consensus on complex and competing perspectives, Minimum of 5 years providing strategic analysis and support to program, division and agency equivalent initiatives, Minimum of 3 years with Agile Scrum, use of JIRA, user story development/validation, sprint planning and backlog grooming, Minimum of 2 years with contract compliance and contract change notices, specific to requirements specifications, Minimum of 2 years practicing LEAN principles and supporting LPI initiatives, Minimum of 1 year working with programs governed by state and/or federal regulations]                                                                                                                     |[facilitation, analysis, consensus]               |[English]                      |2021-07-30  |[content workflow software, graphical user interface development software, compliance software]                                                                                                                                                                                                                                    |[atlassian jira, bps compliance]                                                                                                                                                                                                                                                                  |NULL              |NULL                 |[]                      |contract       |needed                     |NY           |NULL          |[collaboration, customer, other, negotiation, teamwork, meeting, up, thinking, scoping, detail, decision, team, attention]|[English]|[{Business Data Analyst Intern, Internet Brands, Inc., 2016-01-01T00:00:00, 2016-01-01T00:00:00, Total Work experience of 4.6 years in Information System and Agile Environment., NULL, NULL}, {IT Admin Intern, Baby deedee, 2016-10-01T00:00:00, 2016-12-01T00:00:00, Deployed and Delivered enhanced web plugins, security services, website data analysis as per business requirements. Collaborated with design, strategy and sales teams to find solutions for client website projects and developing Complex SQL Queries to illustrate possible solutions. Drafted and Managed business stakeholder expectations to achieve scope in complex and challenging environment. Designing website on Drupal, Modifying Data Warehouses & creating detailed reports on research to provide ideas., NULL, NULL}, {Technical Analyst, General Dynamics, 2014-01-01T00:00:00, 2014-01-01T00:00:00, Trained a fresher's batch of 25 engineers and led the cloud platform maintenance team of 30 technical engineers for MS project process which gathered, transformed & performed Analytics data. Managed responsibilities of scoping and business meetings with business stakeholders and then provided support to the business analysis team in preparing business plans and strategies., NULL, NULL}, {Technical System Engineer, Cisco Systems, Inc., 2014-01-01T00:00:00, 2014-01-01T00:00:00, Coordinated system development tasks to include design, integration and formal testing. Created and maintained programmatic and technical documentation to insure efficient planning and execution. Designated as an Active SME for Microsoft Project server Team of 30 technical engineers. Awarded Certificate of Excellence for Outstanding Performance as a Best Support Engineer (2nd & 4th Quarter 2014). Appreciated for designing a Project Blog site for improvising internal process to post their research and publish ongoing service. Valued for developing a site on Cosmos Server which collaborates the data and generates performance reports. Designed and deployed a Data Breach Detection application for Big Data repositories to identify which user has accessed the component in HDFS for a given day, match it against a rules of access table, and created a line in a report for users that do not match the allowed access criteria in a rules access file., NULL, NULL}, {Trainee System Engineer, Vishesh Pvt Ltd, 2013-01-01T00:00:00, 2013-01-01T00:00:00, Assisted engineers and scientists as they create, modify, and test products and processes. Performed extensive research and development during creation phase of product., NULL, NULL}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |2021-07-30     |[data base user interface and query software, spreadsheet software, project management software, presentation software, business intelligence and data analysis software, development environment software, graphics or photo imaging software, customer relationship management crm software, web platform development software, enterprise resource planning erp software, data base management system software]                                                                                                                                    |[microsoft access, microsoft excel, microsoft project, microsoft powerpoint, microsoft sharepoint, tableau, microsoft visual studio, tibco spotfire s+, microsoft visio, salesforce.com salesforce crm, microsoft sql server, amazon web services aws software, drupal, agile product lifecyle management plm, informatica big data]                                                                                                                                                                                                                                                                                            |Master's Degree  |Computer Science & IT|NULL   |Pace University, Seidenberg School of CSIS|[]             |[0.0, 0.16700889, 0.0, 0.0, 0.0]             |full-time                 |not needed        |Not specified      |[0.75890684, 0.5994749, 0.77623796, 0.5573292, 0.5671483]|\n",
      "|RES_RrDNNvMz|JD_RrDNNvMz|2021-07-13   |No Fit|NULL             |Salesforce Business Analyst                         |NULL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |[Gather and document requirements, Process mapping, Collaborate with cross-functional teams and stakeholders, Analyze data and provide actionable insights]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |[communication, collaboration, analytical skills] |[English]                      |2021-07-13  |[customer relationship management crm software, content workflow software]                                                                                                                                                                                                                                                         |[salesforce software, atlassian jira, salesforce]                                                                                                                                                                                                                                                 |Bachelor's Degree |Business & Management|[]                      |full-time      |not needed                 |Not specified|NULL          |[innovative, detail-oriented, team, client, support, delivery, requirement]                                               |[English]|[{Pro Customer Experience Technician / Data Analyst, Quantitech, 2017-05-01T00:00:00, present, Data Analysis using R. Explored the raw dataset of visa applications, conducted data quality check and implemented data cleaning techniques. Architected a relational database design and transformed the cleaned data into the created model using normalization. Investigated the dataset using data visualization packages in R, illustrating previously unknown insights. Data Mining and Analysis on Lower Back Ache Data using R. Examined the characteristics of data, normalized and visualized it to explore the algorithms that fit it best. Performed Principle Component Analysis and employed decision tree, SVM and Neural network algorithms. Engineered and analyzed the predictive models and improved them using ensemble stack method. Data warehousing and Report writing for the Adventure works Database. Created SQL Server data warehouse using Postgres, Oracle, MySQL and SQL Server as sources. Extracted, Transformed and Loaded data into data warehouse using Talend Studio/ SSIS with error handling and SCD. Statistical and Predictive Analysis of Infant Birth Weight. Performed predictive analysis and data visualization using R Studio and R Shiny. Analyzed various factors that may contribute to a baby's birth weight to identify risk factors associated. Database design for Cellular Service Provider. Designed and created database for sample cellular service provider company using MySQL. Developed front end using PHP, HTML and JavaScript and drew data patterns using Tableau. Collected and Analyzed data for ITS owned laptop usage and devised inventory system there by reducing the cost by 35%. Assisted over 1000 clients from the Northeastern community with technical support for around 10 University applications., NULL, NULL}, {Data Process Engineer Intern, Allegro Microsystems LLC, 2017-06-01T00:00:00, 2017-12-01T00:00:00, Volunteered in modifying existing queries to avoid bottlenecks and automated them, reducing manual effort by 20%. Optimized reports to reduce the loading time, introduced drill down/through reports hence reducing report counts by 60%. Developed KPI reports in team level and coordinated in enhancing team structure and goals. Identified a wrong entry to database, constructed a procedure to replace it and a trigger to notify any further wrong entry., NULL, NULL}, {Systems Engineer, Tata Consultancy Services Limited, 2012-08-01T00:00:00, 2016-08-01T00:00:00, 4+ years of data analysis and BI experience in CloudPlus, ITSM Product incubated in the R&D department of Tata Consultancy Services. Standardized and supervised report delivery process, achieved the goal to reduce delivery time from 2 weeks to 3 business days. Generated generic report templates with the help of requirement gathering from customers and market study in BI tools. Initiated to assist customers with identifying KPI reports using custom reporting tools (BIRT and in-house reporting framework). Led a team for database migration from PostgreSQL to MySQL and fabricated the data warehouse for reporting module. Involved in end to end BI implementation by managing delivery and covering the entire BI lifecycle utilizing agile methodologies. Acted as a technical liaison across 5 business programs and business stakeholders for application support., NULL, NULL}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |2021-07-13     |[object or component oriented development software, data base user interface and query software, analytical or scientific software, document management software, development environment software, enterprise application integration software, business intelligence and data analysis software, data mining software, data base reporting software, industrial control software, data base management system software, object oriented data base management software, enterprise resource planning erp software, web platform development software]|[r, structured query language sql, data analysis software, data visualization software, data warehousing software, adeptia etl suite, talend open studio, microsoft sql server integration services ssis, tableau, bing for power bi, actuate eclipse birt, micosoft sql server analysis services ssas, camstar manufacturing execution system mes, oracle pl/sql, microsoft transact-structural query language t-sql, mysql, postgresql, oracle e-business suite, hypertext preprocessor php, hypertext markup language html, javascript, ibm spss statistics, database design software, agile product lifecyle management plm]|Others           |Engineering (General)|3.7    |Northeastern University-Boston, MA        |[]             |[4.6680355, 0.5010267, 4.0]                  |full-time                 |needed            |Not specified      |[0.6716908, 0.5744712, 0.56427103]                       |\n",
      "+------------+-----------+-------------+------+-----------------+----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------+-------------------------------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+---------------------+------------------------+---------------+---------------------------+-------------+--------------+--------------------------------------------------------------------------------------------------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+---------------------+-------+------------------------------------------+---------------+---------------------------------------------+--------------------------+------------------+-------------------+---------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Education Gold Processing\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Loading silver layer data \n",
    "silver_df = spark.read.parquet(\"datamart/silver/combined_resume_jd/*.parquet\")\n",
    "\n",
    "# Structure Silver Data\n",
    "from utils.spark_utils import pyspark_df_info\n",
    "pyspark_df_info(silver_df)\n",
    "silver_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c32412-c873-4a3f-a392-0da0ed420370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in 'required_edu_level': 1863\n",
      "Null values in 'highest_edu_level': 462\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Assuming silver_df is your PySpark DataFrame\n",
    "null_counts = silver_df.select(\n",
    "    sum(col(\"required_edu_level\").isNull().cast(\"int\")).alias(\"null_count_required_edu_level\"), # Alias is 'null_count_required_edu_level'\n",
    "    sum(col(\"edu_highest_level\").isNull().cast(\"int\")).alias(\"null_count_highest_edu_level\")\n",
    ").collect()[0]\n",
    "\n",
    "# --- This is the line that needs to be corrected ---\n",
    "print(f\"Null values in 'required_edu_level': {null_counts['null_count_required_edu_level']}\")\n",
    "# --- Make sure the key matches the alias exactly: ^ here\n",
    "\n",
    "print(f\"Null values in 'highest_edu_level': {null_counts['null_count_highest_edu_level']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb5d5e7-c036-4a08-bfb4-4dc199a2eb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct required_edu_level values:\n",
      "7\n",
      "High School : 170\n",
      "Master's Degree : 1069\n",
      "Bachelor's Degree : 2984\n",
      "Associate's Degree : 23\n",
      "Others : 81\n",
      "Doctorate : 51\n",
      "None : 0\n",
      "Distinct highest_edu_level values:\n",
      "7\n",
      "High School : 395\n",
      "Master's Degree : 2365\n",
      "Bachelor's Degree : 2147\n",
      "Associate's Degree : 426\n",
      "Others : 296\n",
      "Doctorate : 150\n",
      "None : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Distinct required_edu_level values:\")\n",
    "print(silver_df.select('required_edu_level').distinct().count())\n",
    "\n",
    "\n",
    "\n",
    "for edu_level in silver_df.select('required_edu_level').distinct().collect():\n",
    "    c = silver_df.filter(silver_df.required_edu_level == edu_level.required_edu_level).count()\n",
    "    print(f\"{edu_level.required_edu_level} : {c}\")\n",
    "\n",
    "print(\"Distinct highest_edu_level values:\")\n",
    "print(silver_df.select('edu_highest_level').distinct().count())\n",
    "\n",
    "for edu_level in silver_df.select('edu_highest_level').distinct().collect():\n",
    "    c = silver_df.filter(silver_df.edu_highest_level == edu_level.edu_highest_level).count()\n",
    "    print(f\"{edu_level.edu_highest_level} : {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d8b2cd4-bb68-4571-81b7-fdc621d96154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of new education features:\n",
      "+------------+-----------+-----------------+------------------+----------------+-----------------+--------------+---------+\n",
      "|resume_id   |job_id     |edu_highest_level|required_edu_level|highest_edu_rank|required_edu_rank|edu_match_flag|edu_score|\n",
      "+------------+-----------+-----------------+------------------+----------------+-----------------+--------------+---------+\n",
      "|RES_s93wTCLp|JD_s93wTCLp|Master's Degree  |NULL              |4               |NULL             |NULL          |NULL     |\n",
      "|RES_Z7yf1tu6|JD_Z7yf1tu6|Master's Degree  |Bachelor's Degree |4               |3                |0             |0.2      |\n",
      "|RES_hAp1XnJZ|JD_hAp1XnJZ|Master's Degree  |NULL              |4               |NULL             |NULL          |NULL     |\n",
      "|RES_zsgGxd2s|JD_zsgGxd2s|Master's Degree  |NULL              |4               |NULL             |NULL          |NULL     |\n",
      "|RES_RrDNNvMz|JD_RrDNNvMz|Others           |Bachelor's Degree |0               |3                |0             |-0.6     |\n",
      "|RES_7rYoKTcb|JD_7rYoKTcb|Doctorate        |Bachelor's Degree |5               |3                |1             |0.4      |\n",
      "|RES_dI74kKQw|JD_dI74kKQw|Bachelor's Degree|NULL              |3               |NULL             |NULL          |NULL     |\n",
      "|RES_AAwaK8NY|JD_AAwaK8NY|Bachelor's Degree|Master's Degree   |3               |4                |0             |-0.2     |\n",
      "|RES_jxAmUMgn|JD_jxAmUMgn|Bachelor's Degree|NULL              |3               |NULL             |NULL          |NULL     |\n",
      "|RES_fTYeywv1|JD_fTYeywv1|Others           |NULL              |0               |NULL             |NULL          |NULL     |\n",
      "|RES_ZZpNAyeK|JD_ZZpNAyeK|Master's Degree  |Bachelor's Degree |4               |3                |0             |0.2      |\n",
      "|RES_l7IXSfv7|JD_l7IXSfv7|Bachelor's Degree|NULL              |3               |NULL             |NULL          |NULL     |\n",
      "|RES_vwWeZTDF|JD_vwWeZTDF|High School      |NULL              |1               |NULL             |NULL          |NULL     |\n",
      "|RES_kL06mNvW|JD_kL06mNvW|Bachelor's Degree|Bachelor's Degree |3               |3                |1             |0.0      |\n",
      "|RES_gLmc70sy|JD_gLmc70sy|High School      |Bachelor's Degree |1               |3                |0             |-0.4     |\n",
      "|RES_iFHwzwmT|JD_iFHwzwmT|Master's Degree  |NULL              |4               |NULL             |NULL          |NULL     |\n",
      "|RES_WEgsEJYj|JD_WEgsEJYj|Bachelor's Degree|Bachelor's Degree |3               |3                |1             |0.0      |\n",
      "|RES_JgJdoXyt|JD_JgJdoXyt|Master's Degree  |NULL              |4               |NULL             |NULL          |NULL     |\n",
      "|RES_EZITWfdb|JD_EZITWfdb|Master's Degree  |NULL              |4               |NULL             |NULL          |NULL     |\n",
      "|RES_fqhdZazg|JD_fqhdZazg|Bachelor's Degree|Master's Degree   |3               |4                |0             |-0.2     |\n",
      "+------------+-----------+-----------------+------------------+----------------+-----------------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Final Gold DataFrame schema:\n",
      "root\n",
      " |-- resume_id: string (nullable = true)\n",
      " |-- job_id: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- fit: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- role_title: string (nullable = true)\n",
      " |-- about_the_company: string (nullable = true)\n",
      " |-- job_responsibilities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- jd_soft_skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- required_language_proficiencies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- job_snapshot: date (nullable = true)\n",
      " |-- jd_hard_skills_general: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- jd_hard_skills_specific: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- required_edu_level: string (nullable = true)\n",
      " |-- required_edu_field: string (nullable = true)\n",
      " |-- required_cert_categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- required_work_authorization: string (nullable = true)\n",
      " |-- job_location: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- soft_skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- experience: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- role: string (nullable = true)\n",
      " |    |    |-- company: string (nullable = true)\n",
      " |    |    |-- date_start: string (nullable = true)\n",
      " |    |    |-- date_end: string (nullable = true)\n",
      " |    |    |-- role_description: string (nullable = true)\n",
      " |    |    |-- snapshot_date: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |-- resume_snapshot: string (nullable = true)\n",
      " |-- hard_skills_general: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hard_skills_specific: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- edu_highest_level: string (nullable = true)\n",
      " |-- edu_field: string (nullable = true)\n",
      " |-- edu_gpa: float (nullable = true)\n",
      " |-- edu_institution: string (nullable = true)\n",
      " |-- cert_categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- YoE_list: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- employment_type_preference: string (nullable = true)\n",
      " |-- work_authorization: string (nullable = true)\n",
      " |-- location_preference: string (nullable = true)\n",
      " |-- exp_sim_list: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- edu_match_flag: integer (nullable = true)\n",
      " |-- edu_score: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a SparkSession named 'spark' and your DataFrame 'silver_df' is loaded.\n",
    "# For example:\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"FeatureEngineering\").getOrCreate()\n",
    "# silver_df = spark.read.load(...) # Load your silver data\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# --- Step 1: Define the Ordinal Mapping ---\n",
    "# This Python dictionary holds the ordinal mapping we've defined.\n",
    "edu_level_mapping = {\n",
    "    'Others': 0,\n",
    "    'High School': 1,\n",
    "    'Associate\\'s Degree': 2,\n",
    "    'Bachelor\\'s Degree': 3,\n",
    "    'Master\\'s Degree': 4,\n",
    "    'Doctorate': 5\n",
    "}\n",
    "# Note: 'None' is intentionally left out. It will become null, which we'll handle.\n",
    "\n",
    "# --- Step 2: Create a Spark Mapping Expression ---\n",
    "# We convert the Python dictionary into a Spark mapping expression by first flattening it\n",
    "# into a list of [key1, value1, key2, value2, ...]. This is a more robust approach.\n",
    "flat_map_list = [item for sublist in edu_level_mapping.items() for item in sublist]\n",
    "mapping_expr = F.create_map([F.lit(x) for x in flat_map_list])\n",
    "\n",
    "# --- Step 3: Add Ordinal Rank Columns to the DataFrame ---\n",
    "# We create two new temporary columns that hold the integer rank for the candidate's\n",
    "# education and the job's required education.\n",
    "# The `mapping_expr[F.col(...)]` syntax performs the lookup.\n",
    "gold_df = silver_df.withColumn(\n",
    "    \"highest_edu_rank\",\n",
    "    mapping_expr[F.col(\"edu_highest_level\")].cast(IntegerType())\n",
    ").withColumn(\n",
    "    \"required_edu_rank\",\n",
    "    mapping_expr[F.col(\"required_edu_level\")].cast(IntegerType())\n",
    ")\n",
    "\n",
    "# --- Step 4: Create the 'edu_match_flag' Feature (Updated Null Logic) ---\n",
    "# This feature indicates if the education level is a match, with special handling for nulls.\n",
    "# - A candidate with a Doctorate (rank 5) is always considered a match (1).\n",
    "# - If both levels are present, it's a match (1) only if they are exactly equal.\n",
    "# - If both levels are present and not equal, it's not a match (0).\n",
    "# - Otherwise (one is null and candidate is not a Doctorate), the outcome is null.\n",
    "gold_df = gold_df.withColumn(\n",
    "    \"edu_match_flag\",\n",
    "    F.when(F.col(\"highest_edu_rank\") == 5, 1)\n",
    "     .when(F.col(\"highest_edu_rank\") == F.col(\"required_edu_rank\"), 1)\n",
    "     .when(F.col(\"highest_edu_rank\").isNotNull() & F.col(\"required_edu_rank\").isNotNull(), 0)\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "\n",
    "# --- Step 5: Create the 'edu_score' Feature (Updated Null Logic) ---\n",
    "# This implements the symmetric scoring logic with new exceptions for null values.\n",
    "# - If candidate's education is null, score is null.\n",
    "# - A Doctorate (rank 5) vs a null requirement is a perfect match (0.0).\n",
    "# - A non-Doctorate vs a null requirement is an unknown match (null).\n",
    "# - Otherwise, the score is calculated based on the rank difference.\n",
    "max_rank_diff = float(max(edu_level_mapping.values()) - min(edu_level_mapping.values())) # This is 5.0\n",
    "\n",
    "gold_df = gold_df.withColumn(\n",
    "    \"edu_score\",\n",
    "    F.when(\n",
    "        F.col(\"highest_edu_rank\").isNull(),\n",
    "        None\n",
    "    ).when(\n",
    "        (F.col(\"highest_edu_rank\") == 5) & F.col(\"required_edu_rank\").isNull(),\n",
    "        0.0\n",
    "    ).when(\n",
    "        F.col(\"required_edu_rank\").isNull(),\n",
    "        None\n",
    "    ).otherwise(\n",
    "        (F.col(\"highest_edu_rank\") - F.col(\"required_edu_rank\")) / max_rank_diff\n",
    "    ).cast(FloatType())\n",
    ")\n",
    "\n",
    "# --- Step 6: Handle Null Values (Optional but Recommended) ---\n",
    "# Our logic correctly produces nulls where education info was missing.\n",
    "# For many models, you need to fill these. A common strategy is to fill with 0,\n",
    "# assuming no information means no match.\n",
    "# gold_df = gold_df.na.fill(value=0, subset=[\"edu_match_flag\", \"edu_score\"])\n",
    "\n",
    "# --- Step 7: Verify the Results and Clean Up ---\n",
    "# Select the relevant columns to see the result of our transformations.\n",
    "print(\"Verification of new education features:\")\n",
    "gold_df.select(\n",
    "    \"resume_id\",\n",
    "    \"job_id\",\n",
    "    \"edu_highest_level\",\n",
    "    \"required_edu_level\",\n",
    "    \"highest_edu_rank\",\n",
    "    \"required_edu_rank\",\n",
    "    \"edu_match_flag\",\n",
    "    \"edu_score\"\n",
    ").show(20, truncate=False)\n",
    "\n",
    "# Finally, you can drop the intermediate rank columns to keep the gold table clean.\n",
    "final_gold_df = gold_df.drop(\"highest_edu_rank\", \"required_edu_rank\")\n",
    "\n",
    "print(\"\\nFinal Gold DataFrame schema:\")\n",
    "final_gold_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41bd48-d021-4d89-8528-b35f4acc68bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5beccbf2-3c0d-48bc-8ffd-6fab24fcad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This script contains functions to generate gold-level education features for the \n",
    "# resume-to-job-description matching project.\n",
    "# \"\"\"\n",
    "\n",
    "# from pyspark.sql import DataFrame, SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# # --- Configuration: Define the Ordinal Mapping ---\n",
    "# # This dictionary holds the ordinal mapping for education levels.\n",
    "# # It's defined globally as it's a fixed business rule.\n",
    "# EDU_LEVEL_MAPPING = {\n",
    "#     'Others': 0,\n",
    "#     'High School': 1,\n",
    "#     'Associate\\'s Degree': 2,\n",
    "#     'Bachelor\\'s Degree': 3,\n",
    "#     'Master\\'s Degree': 4,\n",
    "#     'Doctorate': 5\n",
    "# }\n",
    "\n",
    "# def create_education_features(df: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Generates gold-level education features from a silver-level DataFrame.\n",
    "\n",
    "#     This function takes a DataFrame and adds two new columns:\n",
    "#     1.  'edu_match_flag': A binary flag indicating if education levels are an exact match,\n",
    "#         with special handling for nulls and Doctorates.\n",
    "#     2.  'edu_score': A normalized score from -1.0 to 1.0 representing the alignment\n",
    "#         between the candidate's education and the job requirement.\n",
    "\n",
    "#     It then drops the original and intermediate columns used for the calculation.\n",
    "\n",
    "#     Args:\n",
    "#         df (DataFrame): The input silver-level Spark DataFrame.\n",
    "\n",
    "#     Returns:\n",
    "#         DataFrame: A new DataFrame with the added gold features and dropped columns.\n",
    "#     \"\"\"\n",
    "#     # --- Step 1: Create a Spark Mapping Expression ---\n",
    "#     flat_map_list = [item for sublist in EDU_LEVEL_MAPPING.items() for item in sublist]\n",
    "#     mapping_expr = F.create_map([F.lit(x) for x in flat_map_list])\n",
    "\n",
    "#     # --- Step 2: Add Intermediate Ordinal Rank Columns ---\n",
    "#     # These temporary columns hold the integer rank for education levels.\n",
    "#     df_with_ranks = df.withColumn(\n",
    "#         \"highest_edu_rank\",\n",
    "#         mapping_expr[F.col(\"edu_highest_level\")].cast(IntegerType())\n",
    "#     ).withColumn(\n",
    "#         \"required_edu_rank\",\n",
    "#         mapping_expr[F.col(\"required_edu_level\")].cast(IntegerType())\n",
    "#     )\n",
    "\n",
    "#     # --- Step 3: Create the 'edu_match_flag' Feature ---\n",
    "#     df_with_flag = df_with_ranks.withColumn(\n",
    "#         \"edu_match_flag\",\n",
    "#         F.when(F.col(\"highest_edu_rank\") == 5, 1)\n",
    "#          .when(F.col(\"highest_edu_rank\") == F.col(\"required_edu_rank\"), 1)\n",
    "#          .when(F.col(\"highest_edu_rank\").isNotNull() & F.col(\"required_edu_rank\").isNotNull(), 0)\n",
    "#          .otherwise(None)\n",
    "#     )\n",
    "\n",
    "#     # --- Step 4: Create the 'edu_score' Feature ---\n",
    "#     max_rank = max(EDU_LEVEL_MAPPING.values())\n",
    "#     min_rank = min(EDU_LEVEL_MAPPING.values())\n",
    "#     max_rank_diff = float(max_rank - min_rank)\n",
    "\n",
    "#     df_with_score = df_with_flag.withColumn(\n",
    "#         \"edu_score\",\n",
    "#         F.when(F.col(\"highest_edu_rank\").isNull(), None)\n",
    "#          .when((F.col(\"highest_edu_rank\") == 5) & F.col(\"required_edu_rank\").isNull(), 0.0)\n",
    "#          .when(F.col(\"required_edu_rank\").isNull(), None)\n",
    "#          .otherwise((F.col(\"highest_edu_rank\") - F.col(\"required_edu_rank\")) / max_rank_diff)\n",
    "#          .cast(FloatType())\n",
    "#     )\n",
    "\n",
    "#     # --- Step 5: Handle Null Values and Clean Up ---\n",
    "#     # Fill any remaining nulls in our new feature columns with 0.\n",
    "#     # Then, drop the columns as requested.\n",
    "#     final_df = df_with_score.na.fill(value=0, subset=[\"edu_match_flag\", \"edu_score\"])\n",
    "\n",
    "#     columns_to_drop = [\n",
    "#         \"edu_highest_level\", \"required_edu_level\",\n",
    "#         \"highest_edu_rank\", \"required_edu_rank\"\n",
    "#     ]\n",
    "    \n",
    "#     return final_df.drop(*columns_to_drop)\n",
    "\n",
    "# # --- Example Usage Block ---\n",
    "# if __name__ == '__main__':\n",
    "#     # This block demonstrates how to use the function.\n",
    "#     # It will only run when the script is executed directly.\n",
    "    \n",
    "#     spark = SparkSession.builder \\\n",
    "#         .appName(\"EducationFeatureEngineering\") \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "#     # Create a sample DataFrame that mimics your silver_df structure\n",
    "#     sample_data = [\n",
    "#         (\"resume1\", \"job1\", \"Doctorate\", \"Master's Degree\"),   # Overqualified\n",
    "#         (\"resume2\", \"job2\", \"Bachelor's Degree\", \"Bachelor's Degree\"), # Exact match\n",
    "#         (\"resume3\", \"job3\", \"High School\", \"Master's Degree\"),   # Underqualified\n",
    "#         (\"resume4\", \"job4\", \"Doctorate\", None),                 # Null required, should be 1 / 0.0\n",
    "#         (\"resume5\", \"job5\", \"Master's Degree\", None),          # Null required, should be null/null -> 0/0\n",
    "#         (\"resume6\", \"job6\", None, \"Bachelor's Degree\"),        # Null highest, should be null/null -> 0/0\n",
    "#     ]\n",
    "#     columns = [\"resume_id\", \"job_id\", \"edu_highest_level\", \"required_edu_level\"]\n",
    "#     silver_df_sample = spark.createDataFrame(sample_data, columns)\n",
    "\n",
    "#     print(\"--- Sample Silver DataFrame ---\")\n",
    "#     silver_df_sample.show()\n",
    "\n",
    "#     # Apply the feature engineering function\n",
    "#     gold_df_sample = create_education_features(silver_df_sample)\n",
    "\n",
    "#     print(\"\\n--- Resulting Gold DataFrame ---\")\n",
    "#     gold_df_sample.show()\n",
    "    \n",
    "#     print(\"\\n--- Gold DataFrame Schema ---\")\n",
    "#     gold_df_sample.printSchema()\n",
    "\n",
    "#     spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1b02cb-c1d2-4da1-946d-088359160782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in DataFrame: 6241\n",
      "Null values in 'edu_match_flag': 2137\n",
      "Null values in 'edu_score': 2137\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Assuming gold_df is your PySpark DataFrame\n",
    "total_rows = gold_df.count()\n",
    "\n",
    "null_counts = gold_df.select(\n",
    "    sum(col(\"edu_match_flag\").isNull().cast(\"int\")).alias(\"null_count_edu_match_flag\"),\n",
    "    sum(col(\"edu_score\").isNull().cast(\"int\")).alias(\"null_count_edu_score\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Total rows in DataFrame: {total_rows}\")\n",
    "print(f\"Null values in 'edu_match_flag': {null_counts['null_count_edu_match_flag']}\")\n",
    "print(f\"Null values in 'edu_score': {null_counts['null_count_edu_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51afe3-cbf8-4a92-98f4-48a681e35066",
   "metadata": {},
   "source": [
    "# GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed242d1-885c-426e-8900-6fd7bd1978f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Basic Statistics for edu_gpa ---\n",
      "+-------+-----------------+\n",
      "|summary|          edu_gpa|\n",
      "+-------+-----------------+\n",
      "|  count|             1324|\n",
      "|   mean|4.866694829615222|\n",
      "| stddev|7.974494952646127|\n",
      "|    min|             2.49|\n",
      "|    max|            63.28|\n",
      "+-------+-----------------+\n",
      "\n",
      "\n",
      "--- Median Calculation ---\n",
      "Median GPA: 3.759999990463257\n",
      "\n",
      "--- Mode Calculation ---\n",
      "Mode GPA: 4.0\n",
      "\n",
      "--- Top 5 Most Frequent GPA Values ---\n",
      "+-------+-----+\n",
      "|edu_gpa|count|\n",
      "+-------+-----+\n",
      "|    4.0|  220|\n",
      "|    3.6|  137|\n",
      "|    3.5|  134|\n",
      "|    3.9|   84|\n",
      "|   3.86|   58|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- 1. Get Min, Max, and Mean using describe() ---\n",
    "# The describe() function is a quick way to get several key stats.\n",
    "# It computes the count, mean, standard deviation, min, and max.\n",
    "print(\"--- Basic Statistics for edu_gpa ---\")\n",
    "silver_df.select(\"edu_gpa\").describe().show()\n",
    "\n",
    "\n",
    "# --- 2. Calculate the Median ---\n",
    "# The median is the 50th percentile. In Spark, this is calculated using approxQuantile.\n",
    "# The first argument is the column, the second is a list of quantiles (0.5 for median),\n",
    "# and the third is the relative error (0.0 means perfect accuracy).\n",
    "print(\"\\n--- Median Calculation ---\")\n",
    "# approxQuantile returns a list, so we select the first (and only) element.\n",
    "median_gpa = silver_df.approxQuantile(\"edu_gpa\", [0.5], 0.0)[0]\n",
    "print(f\"Median GPA: {median_gpa}\")\n",
    "\n",
    "\n",
    "# --- 3. Calculate the Mode ---\n",
    "# The mode is the most frequently occurring value. To find it, we group by the\n",
    "# GPA values, count the occurrences of each, and find the one with the highest count.\n",
    "print(\"\\n--- Mode Calculation ---\")\n",
    "mode_df = silver_df.filter(F.col(\"edu_gpa\").isNotNull()) \\\n",
    "                   .groupBy(\"edu_gpa\") \\\n",
    "                   .count() \\\n",
    "                   .orderBy(F.col(\"count\").desc())\n",
    "\n",
    "# The mode is the value in the first row of the resulting DataFrame.\n",
    "# .first() returns a Row object, so we access the value by its column name.\n",
    "mode_gpa = mode_df.first()['edu_gpa']\n",
    "print(f\"Mode GPA: {mode_gpa}\")\n",
    "\n",
    "# You can also show the full frequency distribution to check for multiple modes\n",
    "print(\"\\n--- Top 5 Most Frequent GPA Values ---\")\n",
    "mode_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0570a02-6d69-4d7d-846b-aa1835c94a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with GPA > 4: 66\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter the DataFrame to keep only rows where edu_gpa > 4, then count them.\n",
    "gpa_outliers_count = silver_df.filter(F.col(\"edu_gpa\") > 4).count()\n",
    "\n",
    "print(f\"Number of entries with GPA > 4: {gpa_outliers_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f634ccc8-ec3c-497d-a3d6-a2c3042418a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with GPA > 4 and <= 10: 42\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Chain two conditions inside the filter using the '&' (AND) operator\n",
    "gpa_10_scale_count = silver_df.filter(\n",
    "    (F.col(\"edu_gpa\") > 4) & (F.col(\"edu_gpa\") <= 10)\n",
    ").count()\n",
    "\n",
    "print(f\"Number of entries with GPA > 4 and <= 10: {gpa_10_scale_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a353e4e4-d3c9-4d38-b2b9-4c3d8878904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of GPA values greater than 10:\n",
      "[63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875, 63.279998779296875]\n",
      "\n",
      "Total count of such values: 24\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Filter the DataFrame for rows where edu_gpa > 10\n",
    "# 2. Select only the edu_gpa column\n",
    "# 3. Collect the results to the driver as a list of Row objects\n",
    "gpa_outliers_rows = silver_df.filter(F.col(\"edu_gpa\") > 10).select(\"edu_gpa\").collect()\n",
    "\n",
    "# 4. Use a list comprehension to extract the float value from each Row object\n",
    "gpa_outliers_list = [row.edu_gpa for row in gpa_outliers_rows]\n",
    "\n",
    "# Print the final list\n",
    "print(\"List of GPA values greater than 10:\")\n",
    "print(gpa_outliers_list)\n",
    "\n",
    "# You can also print the count for context\n",
    "print(f\"\\nTotal count of such values: {len(gpa_outliers_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008c55e-6fdb-4741-b842-cf53e732bb92",
   "metadata": {},
   "source": [
    "# gpa standardization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bfbdc6-c20f-4582-be9a-52f0f1e85e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains a function to standardize GPA values from different scales\n",
    "into a single 0-4 scale.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def standardize_gpa(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes the 'edu_gpa' column in a DataFrame to a 0-4 scale.\n",
    "\n",
    "    The function applies the following rules:\n",
    "    - Values <= 4 are kept as is.\n",
    "    - Values > 4 and <= 10 are scaled down from a 10-point scale.\n",
    "    - Values > 10 and <= 100 are scaled down from a 100-point scale.\n",
    "    - Null values remain null.\n",
    "    - Values > 100 are considered invalid and are set to null.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input silver-level Spark DataFrame with an 'edu_gpa' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with the 'edu_gpa' column standardized.\n",
    "    \"\"\"\n",
    "    print(\"Standardizing 'edu_gpa' column...\")\n",
    "\n",
    "    # Use a series of 'when' conditions to apply the standardization logic\n",
    "    standardized_df = df.withColumn(\n",
    "        \"edu_gpa\",\n",
    "        F.when(F.col(\"edu_gpa\").isNull(), F.lit(None))  # Keep nulls as null\n",
    "         .when(F.col(\"edu_gpa\") <= 4, F.col(\"edu_gpa\"))  # Already on a 0-4 scale\n",
    "         .when((F.col(\"edu_gpa\") > 4) & (F.col(\"edu_gpa\") <= 10), (F.col(\"edu_gpa\") / 10.0) * 4.0)  # Convert from 10-point scale\n",
    "         .when((F.col(\"edu_gpa\") > 10) & (F.col(\"edu_gpa\") <= 100), (F.col(\"edu_gpa\") / 100.0) * 4.0) # Convert from 100-point scale\n",
    "         .otherwise(F.lit(None))  # Set values > 100 or other edge cases to null\n",
    "         .cast(FloatType()) # Ensure the final column is of FloatType\n",
    "    )\n",
    "\n",
    "    print(\"Standardization complete.\")\n",
    "    return standardized_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbbb46c5-6b67-49f1-b2ea-174f39be33a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing 'edu_gpa' column...\n",
      "Standardization complete.\n",
      "+-----------+------------+-------+\n",
      "|     job_id|   resume_id|edu_gpa|\n",
      "+-----------+------------+-------+\n",
      "|JD_s93wTCLp|RES_s93wTCLp|   NULL|\n",
      "|JD_Z7yf1tu6|RES_Z7yf1tu6|   3.33|\n",
      "|JD_hAp1XnJZ|RES_hAp1XnJZ|   NULL|\n",
      "|JD_zsgGxd2s|RES_zsgGxd2s|   NULL|\n",
      "|JD_RrDNNvMz|RES_RrDNNvMz|    3.7|\n",
      "|JD_7rYoKTcb|RES_7rYoKTcb|    3.6|\n",
      "|JD_dI74kKQw|RES_dI74kKQw|   NULL|\n",
      "|JD_AAwaK8NY|RES_AAwaK8NY|   NULL|\n",
      "|JD_jxAmUMgn|RES_jxAmUMgn|   NULL|\n",
      "|JD_fTYeywv1|RES_fTYeywv1|    3.5|\n",
      "+-----------+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_df = standardize_gpa(silver_df)\n",
    "# Select the \"jobid\" and \"edu_gpa\" columns and show the top 10 rows\n",
    "gold_df.select(\"job_id\", \"resume_id\", \"edu_gpa\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adaf55db-2707-4fb6-8e8d-220f974afc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Basic Statistics for edu_gpa ---\n",
      "+-------+-------------------+\n",
      "|summary|            edu_gpa|\n",
      "+-------+-------------------+\n",
      "|  count|               1324|\n",
      "|   mean| 3.6317906200525623|\n",
      "| stddev|0.37492991501618805|\n",
      "|    min|                1.9|\n",
      "|    max|                4.0|\n",
      "+-------+-------------------+\n",
      "\n",
      "\n",
      "--- Median Calculation ---\n",
      "Median GPA: 3.700000047683716\n",
      "\n",
      "--- Mode Calculation ---\n",
      "Mode GPA: 4.0\n",
      "\n",
      "--- Top 5 Most Frequent GPA Values ---\n",
      "+-------+-----+\n",
      "|edu_gpa|count|\n",
      "+-------+-----+\n",
      "|    4.0|  220|\n",
      "|    3.6|  137|\n",
      "|    3.5|  134|\n",
      "|    3.9|   84|\n",
      "|   3.86|   58|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- 1. Get Min, Max, and Mean using describe() ---\n",
    "# The describe() function is a quick way to get several key stats.\n",
    "# It computes the count, mean, standard deviation, min, and max.\n",
    "print(\"--- Basic Statistics for edu_gpa ---\")\n",
    "gold_df.select(\"edu_gpa\").describe().show()\n",
    "\n",
    "\n",
    "# --- 2. Calculate the Median ---\n",
    "# The median is the 50th percentile. In Spark, this is calculated using approxQuantile.\n",
    "# The first argument is the column, the second is a list of quantiles (0.5 for median),\n",
    "# and the third is the relative error (0.0 means perfect accuracy).\n",
    "print(\"\\n--- Median Calculation ---\")\n",
    "# approxQuantile returns a list, so we select the first (and only) element.\n",
    "median_gpa = gold_df.approxQuantile(\"edu_gpa\", [0.5], 0.0)[0]\n",
    "print(f\"Median GPA: {median_gpa}\")\n",
    "\n",
    "\n",
    "# --- 3. Calculate the Mode ---\n",
    "# The mode is the most frequently occurring value. To find it, we group by the\n",
    "# GPA values, count the occurrences of each, and find the one with the highest count.\n",
    "print(\"\\n--- Mode Calculation ---\")\n",
    "mode_df = gold_df.filter(F.col(\"edu_gpa\").isNotNull()) \\\n",
    "                   .groupBy(\"edu_gpa\") \\\n",
    "                   .count() \\\n",
    "                   .orderBy(F.col(\"count\").desc())\n",
    "\n",
    "# The mode is the value in the first row of the resulting DataFrame.\n",
    "# .first() returns a Row object, so we access the value by its column name.\n",
    "mode_gpa = mode_df.first()['edu_gpa']\n",
    "print(f\"Mode GPA: {mode_gpa}\")\n",
    "\n",
    "# You can also show the full frequency distribution to check for multiple modes\n",
    "print(\"\\n--- Top 5 Most Frequent GPA Values ---\")\n",
    "mode_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f090184-e1c2-482f-a6fc-57bbb800ff1c",
   "metadata": {},
   "source": [
    "# Institution fanciness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e511f8b2-df3a-42dc-8496-a91cb70b7d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct non-null edu_institution values: 5657\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter out nulls, get distinct values, and then count them\n",
    "distinct_institution_count = silver_df.filter(F.col(\"edu_institution\").isNotNull()) \\\n",
    "                                      .distinct() \\\n",
    "                                      .count()\n",
    "\n",
    "print(f\"Number of distinct non-null edu_institution values: {distinct_institution_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7838ad1f-ec70-45f5-ace3-5442a7ced214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw text file from: ./qs_ranking_2021_clean.csv\n",
      "Saving clean Parquet file to: datamart/references/qs_rankings\n",
      "Cleaning complete. A new Parquet file has been saved.\n",
      "\n",
      "Successfully created clean Parquet data in the folder: 'datamart/references/qs_rankings'\n",
      "\n",
      "--- Verifying the first 5 rows and schema of the clean Parquet data ---\n",
      "root\n",
      " |-- Rank: integer (nullable = true)\n",
      " |-- Institution: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+----+--------------------+-------+\n",
      "|Rank|         Institution|Country|\n",
      "+----+--------------------+-------+\n",
      "|   1|Massachusetts Ins...|    USA|\n",
      "|   2| Stanford University|    USA|\n",
      "|   3|  Harvard University|    USA|\n",
      "|   4|California Instit...|    USA|\n",
      "|   5|University of Oxford|     UK|\n",
      "+----+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script takes a raw university rankings CSV file that contains multiple\n",
    "junk header lines, cleans it, enforces a strict schema, and saves the result\n",
    "as a schema-aware Parquet file.\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "def clean_and_save_as_parquet(spark: SparkSession, input_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Reads a raw CSV with junk headers, cleans it, and saves it as a Parquet file.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The active SparkSession.\n",
    "        input_path (str): File path for the raw/dirty CSV.\n",
    "        output_path (str): Folder path to save the clean Parquet file.\n",
    "    \"\"\"\n",
    "    print(f\"Reading raw text file from: {input_path}\")\n",
    "    \n",
    "    # --- Step 1: Read the file as raw text to handle unpredictable headers ---\n",
    "    # This is the most robust way to deal with junk lines.\n",
    "    raw_text_df = spark.read.text(input_path)\n",
    "\n",
    "    # --- Step 2: Filter out all junk/header lines ---\n",
    "    # We keep only the lines that start with a number, which represent the actual data.\n",
    "    # The regex '^\\s*[0-9]' checks if the line starts with optional whitespace followed by a digit.\n",
    "    data_lines_df = raw_text_df.filter(F.col(\"value\").rlike(r\"^\\s*[0-9]\"))\n",
    "\n",
    "    # --- Step 3: Define and Enforce the Schema ---\n",
    "    # Manually define the schema for our data.\n",
    "    final_schema = StructType([\n",
    "        StructField(\"Rank\", IntegerType(), True),\n",
    "        StructField(\"Institution\", StringType(), True),\n",
    "        StructField(\"Country\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Split the text line by commas, trim whitespace, and cast to the correct types.\n",
    "    split_col = F.split(data_lines_df['value'], ',')\n",
    "    clean_df = data_lines_df.select(\n",
    "        F.trim(split_col.getItem(0)).cast(IntegerType()).alias('Rank'),\n",
    "        F.trim(split_col.getItem(1)).alias('Institution'),\n",
    "        F.trim(split_col.getItem(2)).alias('Country')\n",
    "    )\n",
    "    \n",
    "    # Final check to ensure the DataFrame schema matches our defined schema\n",
    "    # (This is implicitly handled by the casts above, but is good practice to be aware of)\n",
    "    \n",
    "    print(f\"Saving clean Parquet file to: {output_path}\")\n",
    "\n",
    "    # --- Step 4: Write the cleaned DataFrame to a Parquet file ---\n",
    "    # Parquet is a columnar, schema-aware format, perfect for reference data.\n",
    "    # .mode(\"overwrite\") will replace the folder if it already exists.\n",
    "    clean_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "    print(\"Cleaning complete. A new Parquet file has been saved.\")\n",
    "\n",
    "\n",
    "# --- Example Usage Block ---\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName(\"RankingsCleaner\").getOrCreate()\n",
    "    \n",
    "    # --- Input and Output Paths ---\n",
    "    raw_csv_path = \"./qs_ranking_2021_clean.csv\"\n",
    "    # The output will be a folder containing the Parquet data.\n",
    "    clean_parquet_path = \"datamart/references/qs_rankings\"\n",
    "    \n",
    "    try:\n",
    "        clean_and_save_as_parquet(spark, raw_csv_path, clean_parquet_path)\n",
    "        print(f\"\\nSuccessfully created clean Parquet data in the folder: '{clean_parquet_path}'\")\n",
    "        \n",
    "        # You can read it back to verify the schema and data\n",
    "        print(\"\\n--- Verifying the first 5 rows and schema of the clean Parquet data ---\")\n",
    "        verified_df = spark.read.parquet(clean_parquet_path)\n",
    "        verified_df.printSchema()\n",
    "        verified_df.show(5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred. Please ensure '{raw_csv_path}' exists and is accessible.\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        \n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9dd57f8-9f3e-4356-b7ef-ede9a1962e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw CSV from: ./qs_ranking_2021.csv\n",
      "Saving clean CSV to: ./qs_ranking_2021_clean_csv\n",
      "Cleaning complete. A new CSV file has been saved.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CsvCleaner\").getOrCreate()\n",
    "    \n",
    "raw_csv_path = \"./qs_ranking_2021.csv\"\n",
    "    # Note: Spark's CSV writer saves to a folder. The actual .csv file will be inside.\n",
    "clean_csv_folder_path = \"./qs_ranking_2021_clean_csv\"\n",
    "\n",
    "\n",
    "clean_rankings_csv(spark, raw_csv_path, clean_csv_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387276c-9394-48cf-b37f-e3f727775c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7dc48047-6a28-4eae-b82a-fc63dadca46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading silver layer data...\n",
      "Running institution feature engineering...\n",
      "\n",
      "--- Final DataFrame with Institution Features (Top 10) ---\n",
      "+------------------------------------------+----------------+\n",
      "|edu_institution                           |institution_tier|\n",
      "+------------------------------------------+----------------+\n",
      "|Rochester Institute of Technology         |Tier 3          |\n",
      "|Syracuse University                       |Tier 3          |\n",
      "|New York Institute of Technology          |Tier 3          |\n",
      "|Pace University, Seidenberg School of CSIS|Tier 3          |\n",
      "|Northeastern University-Boston, MA        |Tier 3          |\n",
      "|Northwestern Polytechnic University       |Tier 3          |\n",
      "|Columbia University                       |Tier 1          |\n",
      "|University of Greenwich-Nairobi           |Tier 3          |\n",
      "|Florida International University          |Tier 3          |\n",
      "|Friends University-Wichita                |Tier 3          |\n",
      "+------------------------------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script creates an 'institution_tier' feature by fuzzy matching against\n",
    "a pre-cleaned, schema-enforced Parquet file of university rankings.\n",
    "\"\"\"\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from rapidfuzz import fuzz # Using rapidfuzz for performance\n",
    "import re\n",
    "\n",
    "def clean_institution_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a university name to a standardized format for matching.\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        return None\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'[^\\w\\s]', '', name) # Remove punctuation\n",
    "    \n",
    "    # Remove common words and suffixes\n",
    "    common_words = ['the', 'of', 'and', 'university', 'college', 'institute']\n",
    "    query = name.split()\n",
    "    result_words = [word for word in query if word.lower() not in common_words]\n",
    "    name = ' '.join(result_words)\n",
    "    \n",
    "    name = name.strip() # Remove leading/trailing whitespace\n",
    "    return name\n",
    "\n",
    "def create_institution_features(spark: SparkSession, silver_df: DataFrame, rankings_parquet_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds an 'institution_tier' column to the DataFrame by matching against\n",
    "    a clean Parquet reference file.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The active SparkSession object.\n",
    "        silver_df (DataFrame): The input silver DataFrame.\n",
    "        rankings_parquet_path (str): The file path to the clean QS rankings Parquet folder.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with the new feature added.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Load and Prepare Rankings Data from Parquet (Simplified) ---\n",
    "    # Reading from Parquet is much simpler as it's schema-aware.\n",
    "    rankings_df = spark.read.parquet(rankings_parquet_path)\n",
    "    \n",
    "    clean_name_udf = F.udf(clean_institution_name, StringType())\n",
    "    # We no longer need the 'Country' column for this feature.\n",
    "    rankings_df = rankings_df.select(\n",
    "        F.col(\"Institution\").alias(\"qs_institution\"),\n",
    "        F.col(\"Rank\").alias(\"qs_rank\")\n",
    "    ).withColumn(\"cleaned_qs_name\", clean_name_udf(F.col(\"qs_institution\")))\n",
    "\n",
    "    # --- Step 2: Prepare Resume Data ---\n",
    "    resume_institutions_df = silver_df.select(\"edu_institution\").filter(F.col(\"edu_institution\").isNotNull()).distinct()\n",
    "    resume_institutions_df = resume_institutions_df.withColumn(\"cleaned_resume_name\", clean_name_udf(F.col(\"edu_institution\")))\n",
    "\n",
    "    # --- Step 3: Fuzzy Match to Find the Best Ranked Match ---\n",
    "    \n",
    "    # Define a function for the UDF to return only rank and score\n",
    "    def get_best_match(resume_name, rankings_broadcast):\n",
    "        best_score = 0\n",
    "        best_rank = None\n",
    "        \n",
    "        for row in rankings_broadcast.value:\n",
    "            qs_name = row['cleaned_qs_name']\n",
    "            score = fuzz.token_sort_ratio(resume_name, qs_name)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_rank = row['qs_rank']\n",
    "        \n",
    "        # Only accept matches with a high confidence score\n",
    "        if best_score > 85:\n",
    "            return (best_rank, best_score)\n",
    "            \n",
    "        # If no good match is found, return None/zero values.\n",
    "        return (None, 0)\n",
    "    \n",
    "    rankings_list_broadcast = spark.sparkContext.broadcast(rankings_df.collect())\n",
    "    \n",
    "    # Define schema for the UDF's return type (no country)\n",
    "    match_schema = StructType([\n",
    "        StructField(\"matched_rank\", IntegerType(), True),\n",
    "        StructField(\"match_score\", IntegerType(), True)\n",
    "    ])\n",
    "    get_best_match_udf = F.udf(lambda name: get_best_match(name, rankings_list_broadcast), match_schema)\n",
    "    \n",
    "    matched_institutions_df = resume_institutions_df.withColumn(\n",
    "        \"match_result\", get_best_match_udf(F.col(\"cleaned_resume_name\"))\n",
    "    )\n",
    "    \n",
    "    # Unpack the struct result\n",
    "    matched_institutions_df = matched_institutions_df.select(\n",
    "        \"edu_institution\",\n",
    "        F.col(\"match_result.matched_rank\").alias(\"matched_rank\"),\n",
    "        F.col(\"match_result.match_score\").alias(\"match_score\")\n",
    "    )\n",
    "    \n",
    "    # --- Step 4: Join Match Results back to the Main DataFrame ---\n",
    "    df_with_rank = silver_df.join(matched_institutions_df, \"edu_institution\", \"left\")\n",
    "\n",
    "    # --- Step 5: Create the Final Feature Column ---\n",
    "    # Create institution_tier\n",
    "    df_with_tier = df_with_rank.withColumn(\n",
    "        \"institution_tier\",\n",
    "        F.when(F.col(\"matched_rank\").isNotNull() & (F.col(\"matched_rank\") <= 100), \"Tier 1\")\n",
    "         .when(F.col(\"matched_rank\").isNotNull() & (F.col(\"matched_rank\") <= 500), \"Tier 2\")\n",
    "         .otherwise(\"Tier 3\") # Tier 3 for ranks > 500 OR for unranked/unmatched\n",
    "    )\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    final_df = df_with_tier.drop(\"matched_rank\", \"match_score\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# --- Example Usage Block (Updated to reflect user's notebook flow) ---\n",
    "if __name__ == '__main__':\n",
    "    # You will need to install rapidfuzz: pip install rapidfuzz\n",
    "    \n",
    "    # 1. Start Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Education Gold Processing\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # 2. Loading silver layer data \n",
    "    print(\"Loading silver layer data...\")\n",
    "    silver_df = spark.read.parquet(\"datamart/silver/combined_resume_jd/*.parquet\")\n",
    "    \n",
    "    # 3. Define path to clean reference data\n",
    "    clean_rankings_path = \"datamart/references/qs_rankings\"\n",
    "    \n",
    "    # 4. Run the feature engineering function\n",
    "    print(\"Running institution feature engineering...\")\n",
    "    try:\n",
    "        # Pass the active spark session and the silver_df to the function\n",
    "        gold_df = create_institution_features(spark, silver_df, clean_rankings_path)\n",
    "    \n",
    "        # 5. Show results\n",
    "        print(\"\\n--- Final DataFrame with Institution Features (Top 10) ---\")\n",
    "        gold_df.select(\"edu_institution\", \"institution_tier\").show(10, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred. Please ensure the folder '{clean_rankings_path}' exists and is accessible.\")\n",
    "        print(f\"Error details: {e}\")\n",
    "    \n",
    "    # Stop the session only at the very end of your entire notebook or script.\n",
    "    # spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb51c26c-5a6a-4464-914e-9a3e13a0152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Row Counts per Institution Tier ---\n",
      "+----------------+-----+\n",
      "|institution_tier|count|\n",
      "+----------------+-----+\n",
      "|          Tier 1|  139|\n",
      "|          Tier 2|  228|\n",
      "|          Tier 3| 5874|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by the 'institution_tier' column and count the occurrences\n",
    "tier_counts_df = gold_df.groupBy(\"institution_tier\").count()\n",
    "\n",
    "# For better readability, order the results by tier\n",
    "tier_counts_df = tier_counts_df.orderBy(\"institution_tier\")\n",
    "\n",
    "# Show the final counts\n",
    "print(\"--- Row Counts per Institution Tier ---\")\n",
    "tier_counts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76131ac6-c7ad-49ff-8f6a-59ceaa56a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def show_certification_columns(df: DataFrame, num_rows: int = 10):\n",
    "  \"\"\"\n",
    "  Displays a specified number of rows for the 'required_cert_categories'\n",
    "  and 'cert_categories' columns of a PySpark DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df: The input PySpark DataFrame.\n",
    "    num_rows: The number of rows to display. Defaults to 10.\n",
    "  \"\"\"\n",
    "  if \"required_cert_categories\" in df.columns and \"cert_categories\" in df.columns:\n",
    "    df.select(\"required_cert_categories\", \"cert_categories\").show(num_rows, truncate=False)\n",
    "  else:\n",
    "    print(\"One or both of the specified columns do not exist in the DataFrame.\")\n",
    "    print(\"Available columns are:\", df.columns)\n",
    "\n",
    "# Example usage with your DataFrame named silver_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d614da-8cbc-4452-a8db-d5b7d1069d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+---------------+\n",
      "|required_cert_categories                       |cert_categories|\n",
      "+-----------------------------------------------+---------------+\n",
      "|[]                                             |[]             |\n",
      "|[]                                             |[]             |\n",
      "|[]                                             |[]             |\n",
      "|[]                                             |[]             |\n",
      "|[]                                             |[]             |\n",
      "|[Business Analysis, Agile & Project Management]|[]             |\n",
      "|[ERP & Business Software]                      |[]             |\n",
      "|[]                                             |[]             |\n",
      "|[ERP & Business Software]                      |[]             |\n",
      "|[]                                             |[]             |\n",
      "+-----------------------------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_certification_columns(silver_df , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52130473-3fe5-45a0-b1e4-8245f0c52c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, BooleanType, StructType, StructField\n",
    "from rapidfuzz import fuzz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "397a5248-3500-478f-bf7b-f999eccfadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cert_match(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Creates the 'cert_match' binary flag based on the intersection of\n",
    "    required certifications and the candidate's certifications.\n",
    "    (Internal helper function)\n",
    "    \"\"\"\n",
    "    # Coalesce null arrays to empty arrays to prevent errors\n",
    "    df_with_arrays = df.withColumn(\"req_certs\", F.coalesce(F.col(\"required_cert_categories\"), F.array())) \\\n",
    "                       .withColumn(\"cand_certs\", F.coalesce(F.col(\"cert_categories\"), F.array()))\n",
    "\n",
    "    # Find the intersection of the two certification lists\n",
    "    df_with_intersection = df_with_arrays.withColumn(\n",
    "        \"cert_intersection\", \n",
    "        F.array_intersect(F.col(\"req_certs\"), F.col(\"cand_certs\"))\n",
    "    )\n",
    "\n",
    "    # Apply the logic:\n",
    "    # Match = 1 if (there is at least one common cert) OR (both lists are empty)\n",
    "    # Match = 0 otherwise.\n",
    "    df_with_flag = df_with_intersection.withColumn(\n",
    "        \"cert_match\",\n",
    "        F.when(\n",
    "            (F.size(F.col(\"cert_intersection\")) > 0) | \n",
    "            ((F.size(F.col(\"req_certs\")) == 0) & (F.size(F.col(\"cand_certs\")) == 0)),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Drop the intermediate helper columns\n",
    "    return df_with_flag.drop(\"req_certs\", \"cand_certs\", \"cert_intersection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e6ea7a1-3f57-45c6-a8fd-58cea380f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_df = create_cert_score(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef94f1b8-7047-4745-a118-356970553d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 10 rows for the 'cert_match' column:\n",
      "+----------+\n",
      "|cert_score|\n",
      "+----------+\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import random\n",
    "\n",
    "# # Create a SparkSession (standard boilerplate for a local script)\n",
    "# # In environments like Databricks, the 'spark' session is usually pre-configured.\n",
    "# spark = SparkSession.builder.appName(\"ShowCertMatch\").getOrCreate()\n",
    "\n",
    "# # --- Create a Sample DataFrame `g_df` for Demonstration ---\n",
    "# # In your real use case, your g_df DataFrame would already be loaded.\n",
    "# # We'll create a 'cert_match' column with some example boolean/string values.\n",
    "# match_options = [True, False, True, True, False]\n",
    "# data = [(i, random.choice(match_options)) for i in range(20)]\n",
    "# columns = [\"id\", \"cert_match\"]\n",
    "# g_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "# --- Show the top 10 values for the \"cert_match\" column ---\n",
    "# 1. .select(\"cert_match\") isolates the column you want.\n",
    "# 2. .show(10) displays the first 10 rows of the selection.\n",
    "\n",
    "print(\"Displaying the top 10 rows for the 'cert_match' column:\")\n",
    "g_df.select(\"cert_score\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1484673-567e-4a5b-aa42-e7fdded24f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 'cert_score': 0.72\n",
      "Median of 'cert_score': 1.0\n",
      "Mode of 'cert_score': 1.0 (appeared 4498 times)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Calculate the Mean ---\n",
    "# We use the built-in `mean` aggregate function on the g_df DataFrame.\n",
    "mean_value = g_df.agg(F.mean(\"cert_score\")).first()[0]\n",
    "print(f\"Mean of 'cert_score': {mean_value:.2f}\")\n",
    "\n",
    "# --- 2. Calculate the Median ---\n",
    "# The median is the 50th percentile. We use `approxQuantile` on g_df.\n",
    "# The parameters are: (column, [percentile], relative_error)\n",
    "# A relative error of 0 gives the exact median.\n",
    "# The result is a list, so we take the first element.\n",
    "median_value = g_df.approxQuantile(\"cert_score\", [0.5], 0.0)[0]\n",
    "print(f\"Median of 'cert_score': {median_value}\")\n",
    "\n",
    "\n",
    "# --- 3. Calculate the Mode ---\n",
    "# The mode is the most frequently occurring value.\n",
    "# We find it by grouping, counting, and ordering on the g_df DataFrame.\n",
    "mode_row = g_df.groupBy(\"cert_score\").count().orderBy(F.col(\"count\").desc()).first()\n",
    "mode_value = mode_row[\"cert_score\"]\n",
    "mode_count = mode_row[\"count\"]\n",
    "\n",
    "print(f\"Mode of 'cert_score': {mode_value} (appeared {mode_count} times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87168b6f-fab4-4945-9a8c-9f8a1edb1a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37383618-e41a-43a7-87a0-c75b76890be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
