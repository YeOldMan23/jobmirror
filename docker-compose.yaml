# services:
#   jupyter:
#     build: .
#     container_name: jupyter_lab
#     image: cs611-project:latest
#     ports:
#       - "8080:8888"
#     volumes:
#       - .:/app
#       # - ./utils:/app/utils
#     environment:
#       - JUPYTER_ENABLE_LAB=yes
#       # - CUDA_LAUNCH_BLOCKING=1 # Keep this for debugging CUDA errors
#     # deploy: # <--- ADD THIS SECTION
#     #   resources:
#     #     reservations:
#     #       devices:
#     #         - driver: nvidia
#     #           count: all # Or specify a number, e.g., 1, or specific device_ids: ['0', '1']
#     #           capabilities: [gpu]
#     command: ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root", "--notebook-dir=/app", "--ServerApp.token=''", "--ServerApp.disable_check_xsrf=True"]
services:
  fix-perms:
    image: busybox
    user: root
    command: >
      sh -c "chown -R 50000:0 /opt/airflow && chown -R 50000:0 /opt/airflow/datamart && chown -R 50000:0 /mlflow && chmod -R 2775 /mlflow"
    volumes:
      - airflow_data:/opt/airflow
      - ./mlflow-data:/mlflow
      - ./datamart:/opt/airflow/datamart
    restart: "no"
  airflow-init:
    build:
      context: .              
      dockerfile: Dockerfile.Airflow
    depends_on:
      - fix-perms
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/app/utils
    volumes:
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ./model_train:/opt/airflow/model_train
      - ./model_deploy:/opt/airflow/model_deploy
      - ./model_monitor:/opt/airflow/model_monitor
      - ./utils:/opt/airflow/utils
      - ivy_cache:/root/.ivy2
      - ./datamart:/opt/airflow/datamart
    entrypoint: >
      /bin/bash -c "airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true"

  airflow-webserver:
    build:
      context: .              
      dockerfile: Dockerfile.Airflow
    depends_on:
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/app/utils
    # env_file:
    #   - .env 
    volumes:
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ./model_train:/opt/airflow/model_train
      - ./model_deploy:/opt/airflow/model_deploy
      - ./model_monitor:/opt/airflow/model_monitor
      - ./scripts:/opt/airflow/scripts
      - ./utils:/opt/airflow/utils
      - ./static:/opt/airflow/static
      - ivy_cache:/root/.ivy2
      - ./datamart:/opt/airflow/datamart
      # - ./.env:/opt/airflow/.env
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    command: webserver

  airflow-scheduler:
    build:
      context: .              
      dockerfile: Dockerfile.Airflow
    depends_on:
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/app
    # env_file:
    #   - .env 
    volumes:
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ./model_train:/opt/airflow/model_train
      - ./model_deploy:/opt/airflow/model_deploy
      - ./model_monitor:/opt/airflow/model_monitor
      - ./scripts:/opt/airflow/scripts
      - ./utils:/opt/airflow/utils
      - ./static:/opt/airflow/static
      - ./mlflow-data:/mlflow
      - ivy_cache:/root/.ivy2
      - ./datamart:/opt/airflow/datamart
    command: scheduler

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    command: >
      sh -c "
        chown -R 0:0 /mlflow &&
        mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:////mlflow/mlflow.db --default-artifact-root /mlflow/artifacts
        "
    volumes:
      - ./mlflow-data:/mlflow

volumes:
  airflow_data:
  ivy_cache:


