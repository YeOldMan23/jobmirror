# Define the services (containers) that make up your application.
services:
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.Jupyter
    container_name: jupyter_lab
    image: jupyter/scipy-notebook:latest
    ports:
      - "8080:8888"
    volumes:
      - .:/app
      # - ./utils:/app/utils
    environment:
      - JUPYTER_ENABLE_LAB=yes
      # - CUDA_LAUNCH_BLOCKING=1 # Keep this for debugging CUDA errors
    deploy: # <--- ADD THIS SECTION
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Or specify a number, e.g., 1, or specific device_ids: ['0', '1']
              capabilities: [gpu]
    command: ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root", "--notebook-dir=/app", "--ServerApp.token=''", "--ServerApp.disable_check_xsrf=True"]
  llama:
    build:
      context: .
      dockerfile: Dockerfile.Llama
    container_name: llama_app
    image: llama_app_image
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    environment:
      - CUDA_VISIBLE_DEVICES=0 # Or specify device IDs as needed
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: ["python", "-m", "gunicorn", "--config", "llm/gunicorn.conf.py", "llm.llm_flask:app"]
    restart: always
    networks:
      - my-custom-network
networks:
  my-custom-network:
    driver: bridge

volumes:
  airflow_data:
  ivy_cache:
